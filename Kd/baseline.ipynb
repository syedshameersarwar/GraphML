{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30840,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# import torch\n# print(torch.__version__)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-19T02:27:35.467643Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install torch-scatter torch-sparse torch-scatter torch-geometric ogb  -f https://data.pyg.org/whl/torch-2.5.1+cu121.html","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch_sparse\nimport torch_scatter\nfrom ogb.graphproppred.mol_encoder import BondEncoder, AtomEncoder\nfrom torch import nn as nn\nfrom torch.nn import functional as F\nfrom torch_geometric import nn as nng\nfrom torch_geometric.data import DataLoader\nfrom ogb.graphproppred import PygGraphPropPredDataset\nfrom copy import copy","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class MLP(nn.Module):\n    def __init__(self, in_dim, out_dim):\n        super().__init__()\n        network = [\n            nn.Linear(in_dim, 2 * in_dim),\n            nn.BatchNorm1d(2 * in_dim),\n            nn.ReLU(),\n            nn.Linear(2 * in_dim, out_dim),\n        ]\n        self.network = nn.Sequential(*network)\n\n    def forward(self, x):\n        return self.network(x)\n\n\nclass OGBMolEmbedding(nn.Module):\n    def __init__(self, dim):\n        super().__init__()\n        self.atom_encoder = AtomEncoder(emb_dim=dim)\n        self.bond_encoder = BondEncoder(emb_dim=dim)\n\n    def forward(self, data):\n        data = copy(data)\n        data.x = self.atom_encoder(data.x)\n        data.edge_attr = self.bond_encoder(data.edge_attr)\n        return data\n\n\nclass VNAgg(nn.Module):\n    def __init__(self, dim, train_eps=False, eps=0.0):\n        super().__init__()\n        self.mlp = nn.Sequential(MLP(dim, dim), nn.BatchNorm1d(dim), nn.ReLU())\n        self.train_eps = train_eps\n        self.eps = (\n            nn.Parameter(torch.Tensor([eps])) if train_eps else torch.Tensor([eps])\n        )\n\n    def forward(self, virtual_node, embeddings, batch_idx):\n        if batch_idx.size(0) > 0:\n            sum_embeddings = nng.global_add_pool(embeddings, batch_idx)\n        else:\n            sum_embeddings = torch.zeros_like(virtual_node, device=device)\n        virtual_node = (1 + self.eps) * virtual_node + sum_embeddings\n        virtual_node = self.mlp(virtual_node)\n        return virtual_node\n\n\nclass GlobalPool(nn.Module):\n    def __init__(self, fun):\n        super().__init__()\n        self.fun = getattr(nng, \"global_{}_pool\".format(fun.lower()))\n\n    def forward(self, data):\n        h, batch_idx = data.x, data.batch\n        pooled = self.fun(h, batch_idx, size=data.num_graphs)\n        return pooled","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class ConvBlock(nn.Module):\n    def __init__(\n        self,\n        dim,\n        dropout=0.5,\n        activation=F.relu,\n        virtual_node=False,\n        virtual_node_agg=True,\n        last_layer=False,\n        train_vn_eps=False,\n        vn_eps=0.0,\n    ):\n        super().__init__()\n        self.conv = nng.GINEConv(MLP(dim, dim), train_eps=True)\n        self.bn = nn.BatchNorm1d(dim)\n        self.activation = activation or nn.Identity()\n        self.dropout_ratio = dropout\n        self.last_layer = last_layer\n        self.virtual_node = virtual_node\n        self.virtual_node_agg = virtual_node_agg\n\n        if self.virtual_node and self.virtual_node_agg:\n            self.virtual_node_agg = VNAgg(dim, train_eps=train_vn_eps, eps=vn_eps)\n\n    def forward(self, data):\n        data = copy(data)\n        h, edge_index, edge_attr, batch_idx = (\n            data.x,\n            data.edge_index,\n            data.edge_attr,\n            data.batch,\n        )\n        if self.virtual_node:\n            h = h + data.virtual_node[batch_idx]\n        h = self.conv(h, edge_index, edge_attr)\n        h = self.bn(h)\n        if not self.last_layer:\n            h = self.activation(h)\n        h = F.dropout(h, self.dropout_ratio, training=self.training)\n        if self.virtual_node and self.virtual_node_agg:\n            v = self.virtual_node_agg(data.virtual_node, h, batch_idx)\n            v = F.dropout(v, self.dropout_ratio, training=self.training)\n            data.virtual_nnn.Functionalode = v\n        data.x = h\n        return data","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class GINENetwork(nn.Module):\n    def __init__(\n        self,\n        hidden_dim=100,\n        out_dim=128,\n        num_layers=3,\n        dropout=0.5,\n        virtual_node=False,\n        train_vn_eps=False,\n        vn_eps=0.0,\n    ):\n        super().__init__()\n        convs = [\n            ConvBlock(\n                hidden_dim,\n                dropout=dropout,\n                virtual_node=virtual_node,\n                train_vn_eps=train_vn_eps,\n                vn_eps=vn_eps,\n            )\n            for _ in range(num_layers - 1)\n        ]\n        convs.append(\n            ConvBlock(\n                hidden_dim,\n                dropout=dropout,\n                virtual_node=virtual_node,\n                virtual_node_agg=False,\n                last_layer=True,\n                train_vn_eps=train_vn_eps,\n                vn_eps=vn_eps,\n            )\n        )\n        self.network = nn.Sequential(OGBMolEmbedding(hidden_dim), *convs)\n        self.aggregate = nn.Sequential(\n            GlobalPool(\"mean\"),\n            MLP(hidden_dim, out_dim),\n        )\n\n        self.virtual_node = virtual_node\n        if self.virtual_node:\n            self.v0 = nn.Parameter(torch.zeros(1, hidden_dim), requires_grad=True)\n\n    def forward(self, data):\n        if self.virtual_node:\n            data.virtual_node = self.v0.expand(data.num_graphs, self.v0.shape[-1])\n        H = self.network(data)\n        return self.aggregate(H)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"if torch.cuda.is_available():\n    device = torch.device(\"cuda\")\nelif hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n    # MPS is currently slower than CPU due to missing int64 min/max ops\n    device = torch.device(\"cpu\")\nelse:\n    device = torch.device(\"cpu\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# load molpcba dataset\ndataset = PygGraphPropPredDataset(name=\"ogbg-molpcba\")\n# split dataset into train, valid, and test","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"split_idx = dataset.get_idx_split()\nbatch_size = 100\ntrain_dataloader = DataLoader(\n    dataset[split_idx[\"train\"]], batch_size=batch_size, shuffle=True, num_workers=4\n)\nvalid_dataloader = DataLoader(\n    dataset[split_idx[\"valid\"]], batch_size=batch_size, shuffle=False, num_workers=4\n)\ntest_dataloader = DataLoader(\n    dataset[split_idx[\"test\"]], batch_size=batch_size, shuffle=False, num_workers=4\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def train(\n    dataset,\n    train_dataloader,\n    valid_dataloader,\n    num_layers=5,\n    hidden_dim=400,\n    dropout=0.5,\n    virtual_node=True,\n    train_vn_eps=False,\n    vn_eps=0.0,\n    lr=0.001,\n    epochs=100,\n):\n    output_dim = dataset.num_tasks\n    model = GINENetwork(\n        hidden_dim=hidden_dim,\n        out_dim=output_dim,\n        num_layers=num_layers,\n        dropout=dropout,\n        virtual_node=virtual_node,\n        train_vn_eps=train_vn_eps,\n        vn_eps=vn_eps,\n    ).to(device)\n    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n    criterion = nn.BCEWithLogitsLoss()\n\n    for epoch in range(epochs):\n        model.train()\n        train_loss = 0\n        for batch in train_dataloader:\n            batch = batch.to(device)\n            optimizer.zero_grad()\n            y_pred = model(batch)\n            y_true = batch.y.float()\n            y_available = ~torch.isnan(y_true)\n            loss = criterion(y_pred[y_available], y_true[y_available])\n            loss.backward()\n            optimizer.step()\n            train_loss += loss.item()\n        train_loss /= len(train_dataloader)\n        if epoch % 10 == 0:\n            model.eval()\n            valid_loss = 0\n            for batch in valid_dataloader:\n                batch = batch.to(device)\n                y_pred = model(batch).to(device)\n                y_true = batch.y.float()\n                y_available = ~torch.isnan(y_true)\n                loss = criterion(y_pred[y_available], y_true[y_available])\n                valid_loss += loss.item()\n            valid_loss /= len(valid_dataloader)\n            print(\n                f\"Epoch {epoch+1}, Train Loss: {train_loss:.4f}, Valid Loss: {valid_loss:.4f}\"\n            )\n    return model\n\n\nmodel = train(\n    dataset,\n    train_dataloader,\n    valid_dataloader,\n    num_layers=5,\n    hidden_dim=400,\n    dropout=0.5,\n    virtual_node=True,\n    train_vn_eps=False,\n    vn_eps=0.0,\n    lr=0.001,\n    epochs=100,\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}