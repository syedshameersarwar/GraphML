{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "602fec09-a231-4855-bab7-19410340e532",
   "metadata": {
    "id": "602fec09-a231-4855-bab7-19410340e532"
   },
   "source": [
    "# Exercise 4\n",
    "\n",
    "Due: Tue November 19, 8:00am\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "027e31e5",
   "metadata": {
    "id": "027e31e5"
   },
   "source": [
    "## Node2Vec\n",
    "\n",
    "1. Implement custom dataset that samples pq-walks\n",
    "   - Use the utility function from torch_cluster that actually performs the walks\n",
    "2. Implement Node2Vec module and training\n",
    "   - Node2Vec essentially consists of a torch.Embedding module and a loss function\n",
    "3. Evaluate node classification performance on Cora\n",
    "4. Evaluate on Link Prediction: Cora, PPI\n",
    "   - use different ways to combine the node two embeddings for link prediction\n",
    "\n",
    "Bonus Question: are the predictions stable wrt to the random seeds of the walks?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "497bd917-87df-4c9c-8d69-7dd0cd90212a",
   "metadata": {
    "id": "497bd917-87df-4c9c-8d69-7dd0cd90212a"
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6YByxDoQRuB4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6YByxDoQRuB4",
    "outputId": "8cd7286d-0050-46ad-875c-8612cbd85701"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5.1+cpu\n",
      "Looking in links: https://data.pyg.org/whl/torch-2.5.1+cpu.html\n",
      "Collecting torch-scatter\n",
      "  Downloading https://data.pyg.org/whl/torch-2.5.0%2Bcpu/torch_scatter-2.1.2%2Bpt25cpu-cp310-cp310-linux_x86_64.whl (543 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m544.0/544.0 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting torch-sparse\n",
      "  Downloading https://data.pyg.org/whl/torch-2.5.0%2Bcpu/torch_sparse-0.6.18%2Bpt25cpu-cp310-cp310-linux_x86_64.whl (1.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m37.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting torch-cluster\n",
      "  Downloading https://data.pyg.org/whl/torch-2.5.0%2Bcpu/torch_cluster-1.6.3%2Bpt25cpu-cp310-cp310-linux_x86_64.whl (785 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m785.3/785.3 kB\u001b[0m \u001b[31m37.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting torch-geometric\n",
      "  Downloading torch_geometric-2.6.1-py3-none-any.whl.metadata (63 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.1/63.1 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from torch-sparse) (1.13.1)\n",
      "Collecting aiohttp (from torch-geometric)\n",
      "  Downloading aiohttp-3.11.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (2024.10.0)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.1.4)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (1.26.4)\n",
      "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (5.9.5)\n",
      "Requirement already satisfied: pyparsing in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.2.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (2.32.3)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (4.66.6)\n",
      "Collecting aiohappyeyeballs>=2.3.0 (from aiohttp->torch-geometric)\n",
      "  Downloading aiohappyeyeballs-2.4.3-py3-none-any.whl.metadata (6.1 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp->torch-geometric)\n",
      "  Downloading aiosignal-1.3.1-py3-none-any.whl.metadata (4.0 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (24.2.0)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp->torch-geometric)\n",
      "  Downloading frozenlist-1.5.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp->torch-geometric)\n",
      "  Downloading multidict-6.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.0 kB)\n",
      "Collecting propcache>=0.2.0 (from aiohttp->torch-geometric)\n",
      "  Downloading propcache-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp->torch-geometric)\n",
      "  Downloading yarl-1.17.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (66 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.6/66.6 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting async-timeout<6.0,>=4.0 (from aiohttp->torch-geometric)\n",
      "  Downloading async_timeout-5.0.1-py3-none-any.whl.metadata (5.1 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch-geometric) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (2024.8.30)\n",
      "Requirement already satisfied: typing-extensions>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from multidict<7.0,>=4.5->aiohttp->torch-geometric) (4.12.2)\n",
      "Downloading torch_geometric-2.6.1-py3-none-any.whl (1.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m21.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading aiohttp-3.11.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m33.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading aiohappyeyeballs-2.4.3-py3-none-any.whl (14 kB)\n",
      "Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Downloading async_timeout-5.0.1-py3-none-any.whl (6.2 kB)\n",
      "Downloading frozenlist-1.5.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (241 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m241.9/241.9 kB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading multidict-6.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (124 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.6/124.6 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading propcache-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (208 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m208.9/208.9 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading yarl-1.17.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (319 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m319.2/319.2 kB\u001b[0m \u001b[31m19.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: torch-scatter, propcache, multidict, frozenlist, async-timeout, aiohappyeyeballs, yarl, torch-sparse, torch-cluster, aiosignal, aiohttp, torch-geometric\n",
      "Successfully installed aiohappyeyeballs-2.4.3 aiohttp-3.11.2 aiosignal-1.3.1 async-timeout-5.0.1 frozenlist-1.5.0 multidict-6.1.0 propcache-0.2.0 torch-cluster-1.6.3+pt25cpu torch-geometric-2.6.1 torch-scatter-2.1.2+pt25cpu torch-sparse-0.6.18+pt25cpu yarl-1.17.2\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n",
    "!pip install torch-scatter torch-sparse torch-cluster torch-geometric -f https://data.pyg.org/whl/torch-{torch.__version__}.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cac6c9e9-fc1f-4a34-8404-165315a2ebf5",
   "metadata": {
    "id": "cac6c9e9-fc1f-4a34-8404-165315a2ebf5"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch_geometric as pyg\n",
    "from tqdm import tqdm\n",
    "import torch_cluster\n",
    "import sklearn\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from typing import Optional\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "22cff34d-205e-4ebe-a747-a36fa653895a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "22cff34d-205e-4ebe-a747-a36fa653895a",
    "outputId": "1716f07c-f773-4379-e66d-46c24a121fa6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find device\n",
    "if torch.cuda.is_available():  # NVIDIA\n",
    "    device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():  # apple M1/M2\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7c275ba2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7c275ba2",
    "outputId": "fe3bf869-a9c0-4560-e700-dd56006f5492"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.x\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.tx\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.allx\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.y\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.ty\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.ally\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.graph\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.test.index\n",
      "Processing...\n",
      "Done!\n",
      "Downloading https://data.dgl.ai/dataset/ppi.zip\n",
      "Extracting dataset/ppi/ppi.zip\n",
      "Processing...\n",
      "/usr/local/lib/python3.10/dist-packages/networkx/readwrite/json_graph/node_link.py:287: FutureWarning: \n",
      "The default value will be changed to `edges=\"edges\" in NetworkX 3.6.\n",
      "\n",
      "To make this warning go away, explicitly set the edges kwarg, e.g.:\n",
      "\n",
      "  nx.node_link_graph(data, edges=\"links\") to preserve current behavior, or\n",
      "  nx.node_link_graph(data, edges=\"edges\") for forward compatibility.\n",
      "  warnings.warn(\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "cora_dataset = pyg.datasets.Planetoid(root=\"./dataset/cora\", name=\"Cora\")\n",
    "cora = cora_dataset[0]\n",
    "ppi_dataset = pyg.datasets.PPI(root=\"./dataset/ppi\")\n",
    "ppi = ppi_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2caa4604-9029-4dad-b418-29ce7bd15415",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2caa4604-9029-4dad-b418-29ce7bd15415",
    "outputId": "6655b59b-cb5d-48f6-bec2-d86a09ded798"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(x=[2708, 1433], edge_index=[2, 10556], y=[2708], train_mask=[2708], val_mask=[2708], test_mask=[2708])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "058731be-e535-4c13-92e1-42caff76d19f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "058731be-e535-4c13-92e1-42caff76d19f",
    "outputId": "2ba6c23c-144d-46ac-946f-3165e5c2417c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(x=[1767, 50], edge_index=[2, 32318], y=[1767, 121])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ppi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "37ae5939",
   "metadata": {
    "id": "37ae5939"
   },
   "outputs": [],
   "source": [
    "seed = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2206d9b1",
   "metadata": {
    "id": "2206d9b1"
   },
   "outputs": [],
   "source": [
    "def set_seed(seed: int = 42) -> None:\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    # When running on the CuDNN backend, two further options must be set\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    # Set a fixed value for the hash seed\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    print(f\"Random seed set as {seed}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3841892f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3841892f",
    "outputId": "3e683dc6-6cb3-4991-93bf-05569ec235b2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random seed set as 0\n"
     ]
    }
   ],
   "source": [
    "set_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "378d4a84-bda3-484a-87ca-35f95344c594",
   "metadata": {
    "id": "378d4a84-bda3-484a-87ca-35f95344c594"
   },
   "source": [
    "## node2vec embedding training\n",
    "\n",
    "Here the main training and everything on the graph level is happening.\n",
    "\n",
    "It might be a good idea to create a dataset of walks (fixed for the whole training process) first to get the whole training process running before attempting to create a train_loader that on-demand samples those walks on-demand.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bb739f66-a29d-435a-a2e3-66287f1e4422",
   "metadata": {
    "id": "bb739f66-a29d-435a-a2e3-66287f1e4422"
   },
   "outputs": [],
   "source": [
    "class PQWalkDataset(torch.utils.data.Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        data,\n",
    "        walk_length,\n",
    "        walks_per_node=1,\n",
    "        p=1,\n",
    "        q=1,\n",
    "        num_negative_samples=1,\n",
    "    ):\n",
    "        self.data = data\n",
    "        # check if edge_label_index is present\n",
    "        if hasattr(self.data, \"edge_label_index\"):\n",
    "            self.edge_index = self.data.edge_label_index\n",
    "        else:\n",
    "            self.edge_index = self.data.edge_index\n",
    "        self.walk_length = walk_length - 1\n",
    "        self.walks_per_node = walks_per_node\n",
    "        self.num_nodes = self.data.num_nodes\n",
    "        self.p = p\n",
    "        self.q = q\n",
    "        self.num_negative_samples = num_negative_samples\n",
    "\n",
    "        self._start_nodes = torch.arange(self.num_nodes).repeat(self.walks_per_node)\n",
    "        self._negative_start_nodes = torch.arange(self.num_nodes).repeat(\n",
    "            self.walks_per_node * self.num_negative_samples\n",
    "        )\n",
    "        self._pos_samples = self._get_pos_samples()\n",
    "        self._neg_samples = self._get_neg_samples()\n",
    "\n",
    "    def _get_pos_samples(self):\n",
    "        return torch_cluster.random_walk(\n",
    "            self.edge_index[0],\n",
    "            self.edge_index[1],\n",
    "            start=self._start_nodes,\n",
    "            walk_length=self.walk_length,\n",
    "            p=self.p,\n",
    "            q=self.q,\n",
    "        )\n",
    "\n",
    "    def _get_neg_samples(self):\n",
    "        negative_samples = torch.randint(\n",
    "            0, self.num_nodes, (self._negative_start_nodes.shape[0], self.walk_length)\n",
    "        )\n",
    "        negative_samples = torch.cat(\n",
    "            [self._negative_start_nodes.view(-1, 1), negative_samples], dim=-1\n",
    "        )\n",
    "        return negative_samples\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._pos_samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        walk = self._pos_samples[idx]\n",
    "        neg_sample = self._neg_samples[idx]\n",
    "        return walk, neg_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7e5ca47f",
   "metadata": {
    "id": "7e5ca47f"
   },
   "outputs": [],
   "source": [
    "class PQWalkIterableDataset(torch.utils.data.IterableDataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        data,\n",
    "        walk_length=10,\n",
    "        walks_per_node=10,\n",
    "        p=1,\n",
    "        q=1,\n",
    "        num_negative_samples=1,\n",
    "    ):\n",
    "        self.data = data\n",
    "        # check if edge_label_index is present\n",
    "        if hasattr(self.data, \"edge_label_index\"):\n",
    "            self.edge_index = self.data.edge_label_index\n",
    "        else:\n",
    "            self.edge_index = self.data.edge_index\n",
    "        self.walk_length = walk_length - 1\n",
    "        self.walks_per_node = walks_per_node\n",
    "        self.num_nodes = self.data.num_nodes\n",
    "        self.p = p\n",
    "        self.q = q\n",
    "        self.num_negative_samples = num_negative_samples\n",
    "\n",
    "    def _generate_negative_samples(self, start_nodes):\n",
    "        # Repeat nodes for each negative sample\n",
    "        nodes = start_nodes.repeat(self.num_negative_samples)\n",
    "\n",
    "        # Generate random walks for negative samples\n",
    "        rw = torch.randint(\n",
    "            self.num_nodes,\n",
    "            (nodes.size(0), self.walk_length),\n",
    "            dtype=nodes.dtype,\n",
    "            device=nodes.device,\n",
    "        )\n",
    "        # Concatenate start nodes with random walks\n",
    "        rw = torch.cat([nodes.view(-1, 1), rw], dim=-1)\n",
    "        return rw\n",
    "\n",
    "    def __iter__(self):\n",
    "        worker_info = torch.utils.data.get_worker_info()\n",
    "        worker_id = 0 if worker_info is None else worker_info.id\n",
    "        num_workers = 1 if worker_info is None else worker_info.num_workers\n",
    "\n",
    "        # Calculate nodes per worker\n",
    "        nodes_per_worker = self.num_nodes // num_workers\n",
    "        start_node = worker_id * nodes_per_worker\n",
    "        end_node = (\n",
    "            start_node + nodes_per_worker\n",
    "            if worker_id < num_workers - 1\n",
    "            else self.num_nodes\n",
    "        )\n",
    "        # worker_nodes = end_node - start_node\n",
    "\n",
    "        # Generate start nodes array that ensures walks_per_node samples for each node\n",
    "        start_nodes = torch.arange(start_node, end_node).repeat_interleave(\n",
    "            self.walks_per_node\n",
    "        )\n",
    "        total_walks = len(start_nodes)\n",
    "\n",
    "        # Shuffle all start nodes\n",
    "        perm = torch.randperm(total_walks)\n",
    "        start_nodes = start_nodes[perm]\n",
    "\n",
    "        walks = torch_cluster.random_walk(\n",
    "            self.edge_index[0],\n",
    "            self.edge_index[1],\n",
    "            start=start_nodes,\n",
    "            walk_length=self.walk_length,\n",
    "            p=self.p,\n",
    "            q=self.q,\n",
    "        )\n",
    "        neg_samples = self._generate_negative_samples(start_nodes)\n",
    "\n",
    "        for walk, neg_sample in zip(walks, neg_samples):\n",
    "            yield walk, neg_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0a63e7cf-74d9-4d52-b83a-62d98ee54f29",
   "metadata": {
    "id": "0a63e7cf-74d9-4d52-b83a-62d98ee54f29"
   },
   "outputs": [],
   "source": [
    "class Node2Vec(torch.nn.Module):\n",
    "    def __init__(self, embedding_dim: int, num_nodes: int):\n",
    "        super(Node2Vec, self).__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.num_nodes = num_nodes\n",
    "        self.embedding = torch.nn.Embedding(num_nodes, embedding_dim)\n",
    "        self._EPS = 1e-15\n",
    "\n",
    "    def loss(self, pos_sample, neg_sample):\n",
    "        assert torch.equal(pos_sample[:, 0], neg_sample[:, 0])\n",
    "        start_nodes = pos_sample[:, 0]\n",
    "        pos_sample_rest = pos_sample[:, 1:].contiguous()\n",
    "        neg_sample_rest = neg_sample[:, 1:].contiguous()\n",
    "\n",
    "        h_start = self.embedding(start_nodes).view(\n",
    "            pos_sample.shape[0], 1, self.embedding_dim\n",
    "        )\n",
    "        h_rest = self.embedding(pos_sample_rest).view(\n",
    "            pos_sample.shape[0], -1, self.embedding_dim\n",
    "        )\n",
    "\n",
    "        out = (h_start * h_rest).sum(dim=-1).view(-1)\n",
    "        pos_loss = -torch.log(torch.sigmoid(out) + self._EPS).mean()\n",
    "\n",
    "        h_start = self.embedding(start_nodes).view(\n",
    "            neg_sample.shape[0], 1, self.embedding_dim\n",
    "        )\n",
    "        h_rest = self.embedding(neg_sample_rest.view(-1)).view(\n",
    "            neg_sample.shape[0], -1, self.embedding_dim\n",
    "        )\n",
    "\n",
    "        out = (h_start * h_rest).sum(dim=-1).view(-1)\n",
    "        neg_loss = -torch.log(1 - torch.sigmoid(out) + self._EPS).mean()\n",
    "\n",
    "        return pos_loss + neg_loss\n",
    "\n",
    "    def get_embedding(self):\n",
    "        return self.embedding.weight\n",
    "\n",
    "    def forward(self, pos_sample, neg_sample):\n",
    "        return self.loss(pos_sample, neg_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "647bdf1c-6ebb-462e-82a7-8e57703fa5d2",
   "metadata": {
    "id": "647bdf1c-6ebb-462e-82a7-8e57703fa5d2"
   },
   "outputs": [],
   "source": [
    "def train_node2vec(\n",
    "    data: pyg.data.Data,\n",
    "    walk_length: int,\n",
    "    walks_per_node: int,\n",
    "    embedding_dim: int,\n",
    "    p: float = 1,\n",
    "    q: float = 1,\n",
    "    num_negative_samples: int = 1,\n",
    "    batch_size: int = 32,\n",
    "    lr: float = 0.01,\n",
    "    num_epochs: int = 200,\n",
    "    num_workers: int = 4,\n",
    "):\n",
    "    dataset = PQWalkIterableDataset(\n",
    "        data=data,\n",
    "        walk_length=walk_length,\n",
    "        walks_per_node=walks_per_node,\n",
    "        num_negative_samples=num_negative_samples,\n",
    "        p=p,\n",
    "        q=q,\n",
    "    )\n",
    "\n",
    "    dataloader = torch.utils.data.DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True if device.type == \"cuda\" else False,\n",
    "    )\n",
    "\n",
    "    model = Node2Vec(embedding_dim, data.num_nodes)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "        optimizer,\n",
    "        T_max=num_epochs,  # Full period of cosine annealing\n",
    "        eta_min=1e-5,  # Minimum learning rate\n",
    "    )\n",
    "    model = model.to(device)\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        num_batches = 0\n",
    "\n",
    "        for pos_sample, neg_sample in dataloader:\n",
    "            pos_sample = pos_sample.to(device)\n",
    "            neg_sample = neg_sample.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            loss = model(pos_sample, neg_sample)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            num_batches += 1\n",
    "\n",
    "        # Calculate average loss for this epoch\n",
    "        avg_loss = total_loss / num_batches\n",
    "        scheduler.step()\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            current_lr = optimizer.param_groups[0][\"lr\"]\n",
    "            print(f\"Epoch {epoch+1:02d}, Loss: {avg_loss:.4f}, LR: {current_lr:.6f}\")\n",
    "\n",
    "    return model.get_embedding()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ddcef892",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ddcef892",
    "outputId": "f7609bb3-6a4d-4180-8592-eebfc6c22d8a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Loss: 1.1265, LR: 0.009756\n",
      "Epoch 20, Loss: 1.1144, LR: 0.009046\n",
      "Epoch 30, Loss: 1.1078, LR: 0.007941\n",
      "Epoch 40, Loss: 1.0961, LR: 0.006549\n",
      "Epoch 50, Loss: 1.0871, LR: 0.005005\n",
      "Epoch 60, Loss: 1.0760, LR: 0.003461\n",
      "Epoch 70, Loss: 1.0691, LR: 0.002069\n",
      "Epoch 80, Loss: 1.0629, LR: 0.000964\n",
      "Epoch 100, Loss: 1.0583, LR: 0.000010\n"
     ]
    }
   ],
   "source": [
    "cora_embeddings = train_node2vec(\n",
    "    cora, 100, 10, 128, p=0.5, q=2.0, lr=0.01, num_epochs=100\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f6e8d3b",
   "metadata": {
    "id": "1f6e8d3b"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "50c83c75",
   "metadata": {
    "id": "50c83c75"
   },
   "outputs": [],
   "source": [
    "embedding_dim = cora_embeddings.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bfe71e5-f122-46ff-abdd-7b0fd15d56fc",
   "metadata": {
    "id": "9bfe71e5-f122-46ff-abdd-7b0fd15d56fc"
   },
   "source": [
    "## Node classification performance\n",
    "\n",
    "just a small MLP or even linear layer on the embeddings to predict node classes. Accuracy should be above 60%. Please compare your results to those you achieved with GNNs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3ca95d6c-ed3f-49eb-b733-17a9035db399",
   "metadata": {
    "id": "3ca95d6c-ed3f-49eb-b733-17a9035db399"
   },
   "outputs": [],
   "source": [
    "# as the simple MLP is pretty straightforward\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(embedding_dim, 256),  # Input layer\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(256, 128),  # Hidden layer 2\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(128, cora_dataset.num_classes),  # Output layer\n",
    ")\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2f648c85-bf32-42f0-94f0-19e052f70325",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2f648c85-bf32-42f0-94f0-19e052f70325",
    "outputId": "f63ce23d-f2b6-484e-a848-c20c5c0498f5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Loss: 2.090e-02\n",
      "Epoch 20, Loss: 1.767e-05\n",
      "Epoch 30, Loss: 3.293e-06\n",
      "Epoch 40, Loss: 4.402e-07\n",
      "Epoch 50, Loss: 1.584e-07\n",
      "Epoch 60, Loss: 1.005e-07\n",
      "Epoch 70, Loss: 8.174e-08\n",
      "Epoch 80, Loss: 7.408e-08\n",
      "Epoch 90, Loss: 6.897e-08\n",
      "Epoch 100, Loss: 6.557e-08\n",
      "node classification accuracy for cora: 0.67 (train: 1.00, val: 0.67)\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)  # define an optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss()  # define loss function\n",
    "\n",
    "cora_embeddings = cora_embeddings.to(device)\n",
    "cora = cora.to(device)\n",
    "\n",
    "for epoch in range(100):  # 100 epochs\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    out = model(cora_embeddings[cora.train_mask])  # forward pass\n",
    "    loss = criterion(out, cora.y[cora.train_mask])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # print out loss info\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Epoch {epoch+1}, Loss: {loss.item():.3e}\")\n",
    "\n",
    "\n",
    "def get_accuracy(model, embeddings, y, mask):\n",
    "    out = model(embeddings[mask])\n",
    "    pred = out.argmax(dim=1)\n",
    "    acc = accuracy_score(y[mask].cpu().numpy(), pred.cpu().detach().numpy())\n",
    "    return acc\n",
    "\n",
    "\n",
    "train_acc = get_accuracy(model, cora_embeddings, cora.y, cora.train_mask)\n",
    "val_acc = get_accuracy(model, cora_embeddings, cora.y, cora.val_mask)\n",
    "test_acc = get_accuracy(model, cora_embeddings, cora.y, cora.test_mask)\n",
    "\n",
    "print(\n",
    "    f\"node classification accuracy for cora: {test_acc:.2f} (train: {train_acc:.2f}, val: {val_acc:.2f})\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f7712ab1-4753-4715-abaf-33b666680535",
   "metadata": {
    "id": "f7712ab1-4753-4715-abaf-33b666680535"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c378a139-b47e-49a1-a65b-90e7a96ca165",
   "metadata": {
    "id": "c378a139-b47e-49a1-a65b-90e7a96ca165"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "83edad57-6fa5-466a-b3d7-56244a79f410",
   "metadata": {
    "id": "83edad57-6fa5-466a-b3d7-56244a79f410"
   },
   "source": [
    "## link prediction on trained embeddings\n",
    "\n",
    "this should only train simple MLPs.\n",
    "\n",
    "Note: for link prediction to be worthwhile, one needs to train the embeddings on a subset of the graph (less edges, same nodes) instead of the whole graph.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "290ffae9-7872-441a-9e63-385619295400",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "290ffae9-7872-441a-9e63-385619295400",
    "outputId": "8b105068-f369-4927-aa2a-a113d52d86dc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(x=[2708, 1433], edge_index=[2, 7392], y=[2708], train_mask=[2708], val_mask=[2708], test_mask=[2708], edge_label=[7392], edge_label_index=[2, 7392])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for link prediction, do something like the following\n",
    "link_splitter = pyg.transforms.RandomLinkSplit(is_undirected=True)\n",
    "train_data, val_data, test_data = link_splitter(cora)\n",
    "train_data\n",
    "# the positive and negative edges are in \"edge_label_index\" with \"edge_label\"\n",
    "# indicating whether an edge is a true edge or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1ef3ab50",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1ef3ab50",
    "outputId": "ce2e7123-373c-423d-9434-eb157ccdd022"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(x=[2708, 1433], edge_index=[2, 8446], y=[2708], train_mask=[2708], val_mask=[2708], test_mask=[2708], edge_label=[2110], edge_label_index=[2, 2110])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fc451bdf-859d-4592-9708-43c93aa8fbb1",
   "metadata": {
    "id": "fc451bdf-859d-4592-9708-43c93aa8fbb1"
   },
   "outputs": [],
   "source": [
    "# retrain node2vec on train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc7cdc42-69e8-4320-b5df-c044ea48b52f",
   "metadata": {
    "id": "cc7cdc42-69e8-4320-b5df-c044ea48b52f"
   },
   "outputs": [],
   "source": [
    "def calculate_mrr(embeddings, pos_edge_index, all_edges, mode=\"filtered\", k=None):\n",
    "    \"\"\"\n",
    "    Calculate MRR with different filtering modes\n",
    "\n",
    "    mode: 'raw', 'filtered', or 'more-filtered'\n",
    "    k: if not None, only consider the top k ranks\n",
    "    \"\"\"\n",
    "    mrr_list = []\n",
    "    num_nodes = embeddings.size(0)\n",
    "\n",
    "    # Convert existing edges to set for faster lookup\n",
    "    existing_edges = set(map(tuple, all_edges.t().tolist()))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in range(pos_edge_index.size(1)):\n",
    "            source = pos_edge_index[0, i]\n",
    "            target = pos_edge_index[1, i]\n",
    "\n",
    "            source_emb = embeddings[source].unsqueeze(0)\n",
    "            all_scores = torch.mm(source_emb, embeddings.t()).squeeze()\n",
    "\n",
    "            # Different filtering modes\n",
    "            if mode == \"raw\":\n",
    "                # No filtering - includes all edges (not recommended)\n",
    "                pass\n",
    "\n",
    "            elif mode == \"filtered\":\n",
    "                # Filter out existing edges except the target\n",
    "                for j in range(num_nodes):\n",
    "                    if (source.item(), j) in existing_edges and j != target.item():\n",
    "                        all_scores[j] = float(\"-inf\")\n",
    "\n",
    "            elif mode == \"more-filtered\":\n",
    "                # Filter existing edges and self-loops\n",
    "                for j in range(num_nodes):\n",
    "                    if (\n",
    "                        (source.item(), j) in existing_edges and j != target.item()\n",
    "                    ) or j == source.item():\n",
    "                        all_scores[j] = float(\"-inf\")\n",
    "\n",
    "            sorted_indices = torch.argsort(all_scores, descending=True)\n",
    "            rank = (sorted_indices == target).nonzero().item() + 1\n",
    "\n",
    "            if k is not None and rank > k:\n",
    "                mrr_list.append(0)\n",
    "            else:\n",
    "                mrr_list.append(1.0 / rank)\n",
    "\n",
    "    return sum(mrr_list) / len(mrr_list)\n",
    "\n",
    "\n",
    "def get_link_labels(pos_edge_index, neg_edge_index):\n",
    "    \"\"\"\n",
    "    Creates labels for positive and negative edges\n",
    "    \"\"\"\n",
    "    num_links = pos_edge_index.size(1) + neg_edge_index.size(1)\n",
    "    link_labels = torch.zeros(num_links, dtype=torch.float)\n",
    "    link_labels[: pos_edge_index.size(1)] = 1.0\n",
    "    return link_labels\n",
    "\n",
    "\n",
    "def get_edge_embeddings(embeddings, edge_index, merge_method: str = \"average\"):\n",
    "    \"\"\"\n",
    "    Combine node embeddings to create edge embeddings\n",
    "    \"\"\"\n",
    "    # Get node embeddings for both source and target nodes\n",
    "    src_embeddings = embeddings[edge_index[0]]\n",
    "    dst_embeddings = embeddings[edge_index[1]]\n",
    "\n",
    "    # Different ways to combine the embeddings\n",
    "    if merge_method == \"hadamard\":\n",
    "        edge_embedding = src_embeddings * dst_embeddings\n",
    "    elif merge_method == \"average\":\n",
    "        edge_embedding = (src_embeddings + dst_embeddings) / 2\n",
    "    elif merge_method == \"l1\":\n",
    "        edge_embedding = torch.abs(src_embeddings - dst_embeddings)\n",
    "    elif merge_method == \"l2\":\n",
    "        edge_embedding = (src_embeddings - dst_embeddings) ** 2\n",
    "    return edge_embedding\n",
    "\n",
    "\n",
    "def evaluate_link_prediction(\n",
    "    embeddings,\n",
    "    edge_classifier,\n",
    "    pos_edge_index,\n",
    "    neg_edge_index,\n",
    "    merge_method: str = \"average\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Evaluate link prediction performance\n",
    "    \"\"\"\n",
    "    # Get edge embeddings\n",
    "    with torch.no_grad():  # Don't track gradients for embeddings\n",
    "        pos_edge_embeddings = get_edge_embeddings(\n",
    "            embeddings, pos_edge_index, merge_method\n",
    "        )\n",
    "        neg_edge_embeddings = get_edge_embeddings(\n",
    "            embeddings, neg_edge_index, merge_method\n",
    "        )\n",
    "\n",
    "        # Combine positive and negative edge embeddings\n",
    "        edge_embeddings = torch.cat([pos_edge_embeddings, neg_edge_embeddings], dim=0)\n",
    "\n",
    "        # Create labels\n",
    "        labels = get_link_labels(pos_edge_index, neg_edge_index)\n",
    "\n",
    "    # Evaluate\n",
    "    edge_classifier.eval()\n",
    "    with torch.no_grad():\n",
    "        pred = edge_classifier(edge_embeddings).squeeze()\n",
    "        auc_score = sklearn.metrics.roc_auc_score(labels.cpu(), pred.cpu())\n",
    "        ap_score = sklearn.metrics.average_precision_score(labels.cpu(), pred.cpu())\n",
    "    all_edges = torch.cat([pos_edge_index, neg_edge_index], dim=1)\n",
    "    mrr = calculate_mrr(embeddings, pos_edge_index, all_edges)\n",
    "    return auc_score, ap_score, mrr\n",
    "\n",
    "\n",
    "def train_and_evaluate_link_prediction(\n",
    "    data,\n",
    "    walk_length: int = 100,\n",
    "    walks_per_node: int = 10,\n",
    "    embedding_dim: int = 128,\n",
    "    p: float = 1,\n",
    "    q: float = 1,\n",
    "    lr: float = 0.01,\n",
    "    num_epochs: int = 100,\n",
    "    merge_method: str = \"average\",\n",
    "    num_workers: int = 4,\n",
    "):\n",
    "    link_splitter = pyg.transforms.RandomLinkSplit(is_undirected=True)\n",
    "    train_data, val_data, test_data = link_splitter(data)\n",
    "\n",
    "    embeddings = train_node2vec(\n",
    "        train_data,\n",
    "        walk_length,\n",
    "        walks_per_node,\n",
    "        embedding_dim,\n",
    "        p,\n",
    "        q,\n",
    "        lr=lr,\n",
    "        num_epochs=num_epochs,\n",
    "        num_workers=num_workers,\n",
    "    )\n",
    "    embeddings = embeddings.detach()\n",
    "\n",
    "    # Train a simple classifier\n",
    "    edge_classifier = torch.nn.Sequential(\n",
    "        torch.nn.Linear(embeddings.shape[1], 64),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Linear(64, 1),\n",
    "        torch.nn.Sigmoid(),\n",
    "    ).to(device)\n",
    "\n",
    "    # Train the classifier\n",
    "    optimizer = torch.optim.AdamW(edge_classifier.parameters(), lr=0.01)\n",
    "    criterion = torch.nn.BCELoss()\n",
    "\n",
    "    pos_edge_embeddings = get_edge_embeddings(\n",
    "        embeddings,\n",
    "        train_data.edge_label_index[:, train_data.edge_label == 1],\n",
    "        merge_method,\n",
    "    )\n",
    "    neg_edge_embeddings = get_edge_embeddings(\n",
    "        embeddings,\n",
    "        train_data.edge_label_index[:, train_data.edge_label == 0],\n",
    "        merge_method,\n",
    "    )\n",
    "\n",
    "    # Combine positive and negative edge embeddings\n",
    "    edge_embeddings = torch.cat([pos_edge_embeddings, neg_edge_embeddings], dim=0)\n",
    "\n",
    "    # Create labels\n",
    "    labels = get_link_labels(\n",
    "        train_data.edge_label_index[:, train_data.edge_label == 1],\n",
    "        train_data.edge_label_index[:, train_data.edge_label == 0],\n",
    "    )\n",
    "\n",
    "    edge_embeddings = edge_embeddings.to(device)\n",
    "    labels = labels.to(device)\n",
    "\n",
    "    # Training loop\n",
    "    edge_classifier.train()\n",
    "    for _ in range(100):\n",
    "        optimizer.zero_grad()\n",
    "        out = edge_classifier(edge_embeddings).squeeze()\n",
    "        loss = criterion(out, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Evaluate on validation set\n",
    "    val_auc, val_ap, val_mrr = evaluate_link_prediction(\n",
    "        embeddings,\n",
    "        edge_classifier,\n",
    "        val_data.edge_label_index[:, val_data.edge_label == 1],\n",
    "        val_data.edge_label_index[:, val_data.edge_label == 0],\n",
    "        merge_method,\n",
    "    )\n",
    "\n",
    "    # Evaluate on test set\n",
    "    test_auc, test_ap, test_mrr = evaluate_link_prediction(\n",
    "        embeddings,\n",
    "        edge_classifier,\n",
    "        test_data.edge_label_index[:, test_data.edge_label == 1],\n",
    "        test_data.edge_label_index[:, test_data.edge_label == 0],\n",
    "        merge_method,\n",
    "    )\n",
    "\n",
    "    return embeddings, (val_auc, val_ap, val_mrr), (test_auc, test_ap, test_mrr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e9f11bab-e219-4434-9e2d-16b2e3c702c9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e9f11bab-e219-4434-9e2d-16b2e3c702c9",
    "outputId": "0a5f639a-0c90-40da-dc46-1ac721382cc2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Link prediction on Cora:\n",
      "Epoch 10, Loss: 1.2242, LR: 0.009046\n",
      "Epoch 20, Loss: 1.1255, LR: 0.006549\n",
      "Epoch 30, Loss: 1.0587, LR: 0.003461\n",
      "Epoch 40, Loss: 1.0121, LR: 0.000964\n",
      "Epoch 50, Loss: 1.0016, LR: 0.000010\n",
      "Cora embeddings shape: torch.Size([2708, 128])\n",
      "Cora validation results: AUC: 0.6281, AP: 0.6390, MRR: 0.0127\n",
      "Cora test results: AUC: 0.6248, AP: 0.6466, MRR: 0.0101\n"
     ]
    }
   ],
   "source": [
    "# use those (new) embeddings for link prediction\n",
    "print(\"Link prediction on Cora:\")\n",
    "cora_results = train_and_evaluate_link_prediction(\n",
    "    cora, 100, 10, 128, p=0.5, q=2.0, lr=0.01, num_epochs=50, merge_method=\"average\"\n",
    ")\n",
    "cora_embeddings, cora_val_results, cora_test_results = cora_results\n",
    "print(f\"Cora embeddings shape: {cora_embeddings.shape}\")\n",
    "print(\n",
    "    f\"Cora validation results: AUC: {cora_val_results[0]:.4f}, AP: {cora_val_results[1]:.4f}, MRR: {cora_val_results[2]:.4f}\"\n",
    ")\n",
    "print(\n",
    "    f\"Cora test results: AUC: {cora_test_results[0]:.4f}, AP: {cora_test_results[1]:.4f}, MRR: {cora_test_results[2]:.4f}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4a632698-60fd-40d6-b3c3-d924402f24e5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4a632698-60fd-40d6-b3c3-d924402f24e5",
    "outputId": "d71fd0c4-7758-4d3c-8ad0-980e65bc42a6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Link prediction on PPI:\n",
      "Epoch 10, Loss: 1.3773, LR: 0.009046\n",
      "Epoch 20, Loss: 1.3706, LR: 0.006549\n",
      "Epoch 30, Loss: 1.3634, LR: 0.003461\n",
      "Epoch 40, Loss: 1.3592, LR: 0.000964\n",
      "Epoch 50, Loss: 1.3571, LR: 0.000010\n",
      "PPI embeddings shape: torch.Size([1767, 128])\n",
      "PPI validation results: AUC: 0.8261, AP: 0.8243, MRR: 0.0344\n",
      "PPI test results: AUC: 0.8386, AP: 0.8339, MRR: 0.0281\n"
     ]
    }
   ],
   "source": [
    "print(\"Link prediction on PPI:\")\n",
    "ppi_results = train_and_evaluate_link_prediction(\n",
    "    ppi, 100, 10, 128, p=0.5, q=2.0, lr=0.01, num_epochs=50, merge_method=\"average\"\n",
    ")\n",
    "ppi_embeddings, ppi_val_results, ppi_test_results = ppi_results\n",
    "print(f\"PPI embeddings shape: {ppi_embeddings.shape}\")\n",
    "print(\n",
    "    f\"PPI validation results: AUC: {ppi_val_results[0]:.4f}, AP: {ppi_val_results[1]:.4f}, MRR: {ppi_val_results[2]:.4f}\"\n",
    ")\n",
    "print(\n",
    "    f\"PPI test results: AUC: {ppi_test_results[0]:.4f}, AP: {ppi_test_results[1]:.4f}, MRR: {ppi_test_results[2]:.4f}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c840653f-8348-4792-86fd-59679ee523bf",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c840653f-8348-4792-86fd-59679ee523bf",
    "outputId": "bc589ade-35eb-4d68-ae5b-bd89cf5c72af"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Link prediction on Cora:\n",
      "Merge method: average\n",
      "Epoch 10, Loss: 1.2271, LR: 0.009046\n",
      "Epoch 20, Loss: 1.1106, LR: 0.006549\n",
      "Epoch 30, Loss: 1.0433, LR: 0.003461\n",
      "Epoch 40, Loss: 1.0047, LR: 0.000964\n",
      "Epoch 50, Loss: 0.9892, LR: 0.000010\n",
      "Cora embeddings shape: torch.Size([2708, 128])\n",
      "Cora validation results: AUC: 0.6346, AP: 0.6597, MRR: 0.0096\n",
      "Cora test results: AUC: 0.6259, AP: 0.6477, MRR: 0.0113\n",
      "\n",
      "Merge method: hadamard\n",
      "Epoch 10, Loss: 1.2472, LR: 0.009046\n",
      "Epoch 20, Loss: 1.1346, LR: 0.006549\n",
      "Epoch 30, Loss: 1.0580, LR: 0.003461\n",
      "Epoch 40, Loss: 1.0098, LR: 0.000964\n",
      "Epoch 50, Loss: 1.0015, LR: 0.000010\n",
      "Cora embeddings shape: torch.Size([2708, 128])\n",
      "Cora validation results: AUC: 0.4756, AP: 0.4926, MRR: 0.0139\n",
      "Cora test results: AUC: 0.4506, AP: 0.4603, MRR: 0.0083\n",
      "\n",
      "Merge method: l1\n",
      "Epoch 10, Loss: 1.2442, LR: 0.009046\n",
      "Epoch 20, Loss: 1.1363, LR: 0.006549\n",
      "Epoch 30, Loss: 1.0629, LR: 0.003461\n",
      "Epoch 40, Loss: 1.0175, LR: 0.000964\n",
      "Epoch 50, Loss: 1.0062, LR: 0.000010\n",
      "Cora embeddings shape: torch.Size([2708, 128])\n",
      "Cora validation results: AUC: 0.4790, AP: 0.4860, MRR: 0.0110\n",
      "Cora test results: AUC: 0.5034, AP: 0.4943, MRR: 0.0142\n",
      "\n",
      "Merge method: l2\n",
      "Epoch 10, Loss: 1.2246, LR: 0.009046\n",
      "Epoch 20, Loss: 1.1121, LR: 0.006549\n",
      "Epoch 30, Loss: 1.0493, LR: 0.003461\n",
      "Epoch 40, Loss: 1.0097, LR: 0.000964\n",
      "Epoch 50, Loss: 0.9972, LR: 0.000010\n",
      "Cora embeddings shape: torch.Size([2708, 128])\n",
      "Cora validation results: AUC: 0.5102, AP: 0.5178, MRR: 0.0077\n",
      "Cora test results: AUC: 0.5197, AP: 0.5116, MRR: 0.0078\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# different merge methods on Cora\n",
    "print(\"Link prediction on Cora:\")\n",
    "device = torch.device(\"cpu\")\n",
    "cora = cora.to(device)\n",
    "\n",
    "for merge_method in [\"average\", \"hadamard\", \"l1\", \"l2\"]:\n",
    "    print(\"Merge method:\", merge_method)\n",
    "    cora_results = train_and_evaluate_link_prediction(\n",
    "        cora,\n",
    "        100,\n",
    "        10,\n",
    "        128,\n",
    "        p=0.5,\n",
    "        q=2.0,\n",
    "        lr=0.01,\n",
    "        num_epochs=50,\n",
    "        merge_method=merge_method,\n",
    "    )\n",
    "    cora_embeddings, cora_val_results, cora_test_results = cora_results\n",
    "    print(f\"Cora embeddings shape: {cora_embeddings.shape}\")\n",
    "    print(\n",
    "        f\"Cora validation results: AUC: {cora_val_results[0]:.4f}, AP: {cora_val_results[1]:.4f}, MRR: {cora_val_results[2]:.4f}\"\n",
    "    )\n",
    "    print(\n",
    "        f\"Cora test results: AUC: {cora_test_results[0]:.4f}, AP: {cora_test_results[1]:.4f}, MRR: {cora_test_results[2]:.4f}\"\n",
    "    )\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "96f0649e-7dbc-441b-b019-1f7239a5c5c1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "96f0649e-7dbc-441b-b019-1f7239a5c5c1",
    "outputId": "7e98ec4e-a475-46f6-d06a-3bf56697468e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Link prediction on PPI:\n",
      "Merge method: average\n",
      "Epoch 10, Loss: 1.3693, LR: 0.009046\n",
      "Epoch 20, Loss: 1.3630, LR: 0.006549\n",
      "Epoch 30, Loss: 1.3542, LR: 0.003461\n",
      "Epoch 40, Loss: 1.3485, LR: 0.000964\n",
      "Epoch 50, Loss: 1.3454, LR: 0.000010\n",
      "PPI embeddings shape: torch.Size([1767, 128])\n",
      "PPI validation results: AUC: 0.8397, AP: 0.8385, MRR: 0.0262\n",
      "PPI test results: AUC: 0.8361, AP: 0.8325, MRR: 0.0255\n",
      "\n",
      "Merge method: hadamard\n",
      "Epoch 10, Loss: 1.3746, LR: 0.009046\n",
      "Epoch 20, Loss: 1.3695, LR: 0.006549\n",
      "Epoch 30, Loss: 1.3621, LR: 0.003461\n",
      "Epoch 40, Loss: 1.3575, LR: 0.000964\n",
      "Epoch 50, Loss: 1.3555, LR: 0.000010\n",
      "PPI embeddings shape: torch.Size([1767, 128])\n",
      "PPI validation results: AUC: 0.6517, AP: 0.6749, MRR: 0.0278\n",
      "PPI test results: AUC: 0.6618, AP: 0.6781, MRR: 0.0282\n",
      "\n",
      "Merge method: l1\n",
      "Epoch 10, Loss: 1.3780, LR: 0.009046\n",
      "Epoch 20, Loss: 1.3732, LR: 0.006549\n",
      "Epoch 30, Loss: 1.3660, LR: 0.003461\n",
      "Epoch 40, Loss: 1.3618, LR: 0.000964\n",
      "Epoch 50, Loss: 1.3606, LR: 0.000010\n",
      "PPI embeddings shape: torch.Size([1767, 128])\n",
      "PPI validation results: AUC: 0.6726, AP: 0.6810, MRR: 0.0348\n",
      "PPI test results: AUC: 0.6690, AP: 0.6810, MRR: 0.0400\n",
      "\n",
      "Merge method: l2\n",
      "Epoch 10, Loss: 1.3786, LR: 0.009046\n",
      "Epoch 20, Loss: 1.3712, LR: 0.006549\n",
      "Epoch 30, Loss: 1.3631, LR: 0.003461\n",
      "Epoch 40, Loss: 1.3586, LR: 0.000964\n",
      "Epoch 50, Loss: 1.3576, LR: 0.000010\n",
      "PPI embeddings shape: torch.Size([1767, 128])\n",
      "PPI validation results: AUC: 0.6806, AP: 0.7029, MRR: 0.0253\n",
      "PPI test results: AUC: 0.6885, AP: 0.6952, MRR: 0.0308\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# different merge methods on PPI\n",
    "print(\"Link prediction on PPI:\")\n",
    "for merge_method in [\"average\", \"hadamard\", \"l1\", \"l2\"]:\n",
    "    print(\"Merge method:\", merge_method)\n",
    "    ppi_results = train_and_evaluate_link_prediction(\n",
    "        ppi,\n",
    "        100,\n",
    "        10,\n",
    "        128,\n",
    "        p=0.5,\n",
    "        q=2.0,\n",
    "        lr=0.01,\n",
    "        num_epochs=50,\n",
    "        merge_method=merge_method,\n",
    "        num_workers=0,\n",
    "    )\n",
    "    ppi_embeddings, ppi_val_results, ppi_test_results = ppi_results\n",
    "    print(f\"PPI embeddings shape: {ppi_embeddings.shape}\")\n",
    "    print(\n",
    "        f\"PPI validation results: AUC: {ppi_val_results[0]:.4f}, AP: {ppi_val_results[1]:.4f}, MRR: {ppi_val_results[2]:.4f}\"\n",
    "    )\n",
    "    print(\n",
    "        f\"PPI test results: AUC: {ppi_test_results[0]:.4f}, AP: {ppi_test_results[1]:.4f}, MRR: {ppi_test_results[2]:.4f}\"\n",
    "    )\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "gpuType": "V28",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
