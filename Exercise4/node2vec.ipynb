{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "602fec09-a231-4855-bab7-19410340e532",
      "metadata": {
        "id": "602fec09-a231-4855-bab7-19410340e532"
      },
      "source": [
        "# Exercise 4\n",
        "\n",
        "Due: Tue November 19, 8:00am\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "027e31e5",
      "metadata": {
        "id": "027e31e5"
      },
      "source": [
        "## Node2Vec\n",
        "\n",
        "1. Implement custom dataset that samples pq-walks\n",
        "   - Use the utility function from torch_cluster that actually performs the walks\n",
        "2. Implement Node2Vec module and training\n",
        "   - Node2Vec essentially consists of a torch.Embedding module and a loss function\n",
        "3. Evaluate node classification performance on Cora\n",
        "4. Evaluate on Link Prediction: Cora, PPI\n",
        "   - use different ways to combine the node two embeddings for link prediction\n",
        "\n",
        "Bonus Question: are the predictions stable wrt to the random seeds of the walks?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "497bd917-87df-4c9c-8d69-7dd0cd90212a",
      "metadata": {
        "id": "497bd917-87df-4c9c-8d69-7dd0cd90212a"
      },
      "outputs": [],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "6YByxDoQRuB4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6YByxDoQRuB4",
        "outputId": "ddf5bc3c-98ab-42b5-8014-867de4ceb983"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2.5.1+cpu\n",
            "Looking in links: https://data.pyg.org/whl/torch-2.5.1+cpu.html\n",
            "Collecting torch-scatter\n",
            "  Downloading https://data.pyg.org/whl/torch-2.5.0%2Bcpu/torch_scatter-2.1.2%2Bpt25cpu-cp310-cp310-linux_x86_64.whl (543 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m544.0/544.0 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torch-sparse\n",
            "  Downloading https://data.pyg.org/whl/torch-2.5.0%2Bcpu/torch_sparse-0.6.18%2Bpt25cpu-cp310-cp310-linux_x86_64.whl (1.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torch-cluster\n",
            "  Downloading https://data.pyg.org/whl/torch-2.5.0%2Bcpu/torch_cluster-1.6.3%2Bpt25cpu-cp310-cp310-linux_x86_64.whl (785 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m785.3/785.3 kB\u001b[0m \u001b[31m30.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torch-geometric\n",
            "  Downloading torch_geometric-2.6.1-py3-none-any.whl.metadata (63 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.1/63.1 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from torch-sparse) (1.13.1)\n",
            "Collecting aiohttp (from torch-geometric)\n",
            "  Downloading aiohttp-3.11.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (2024.10.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.1.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (1.26.4)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (5.9.5)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.2.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (4.66.6)\n",
            "Collecting aiohappyeyeballs>=2.3.0 (from aiohttp->torch-geometric)\n",
            "  Downloading aiohappyeyeballs-2.4.3-py3-none-any.whl.metadata (6.1 kB)\n",
            "Collecting aiosignal>=1.1.2 (from aiohttp->torch-geometric)\n",
            "  Downloading aiosignal-1.3.1-py3-none-any.whl.metadata (4.0 kB)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (24.2.0)\n",
            "Collecting frozenlist>=1.1.1 (from aiohttp->torch-geometric)\n",
            "  Downloading frozenlist-1.5.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
            "Collecting multidict<7.0,>=4.5 (from aiohttp->torch-geometric)\n",
            "  Downloading multidict-6.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.0 kB)\n",
            "Collecting propcache>=0.2.0 (from aiohttp->torch-geometric)\n",
            "  Downloading propcache-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
            "Collecting yarl<2.0,>=1.17.0 (from aiohttp->torch-geometric)\n",
            "  Downloading yarl-1.17.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (64 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.8/64.8 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting async-timeout<6.0,>=4.0 (from aiohttp->torch-geometric)\n",
            "  Downloading async_timeout-5.0.1-py3-none-any.whl.metadata (5.1 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch-geometric) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (2024.8.30)\n",
            "Requirement already satisfied: typing-extensions>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from multidict<7.0,>=4.5->aiohttp->torch-geometric) (4.12.2)\n",
            "Downloading torch_geometric-2.6.1-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m21.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aiohttp-3.11.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m49.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aiohappyeyeballs-2.4.3-py3-none-any.whl (14 kB)\n",
            "Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
            "Downloading async_timeout-5.0.1-py3-none-any.whl (6.2 kB)\n",
            "Downloading frozenlist-1.5.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (241 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m241.9/241.9 kB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multidict-6.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (124 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.6/124.6 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading propcache-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (208 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m208.9/208.9 kB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading yarl-1.17.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (318 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m318.7/318.7 kB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torch-scatter, propcache, multidict, frozenlist, async-timeout, aiohappyeyeballs, yarl, torch-sparse, torch-cluster, aiosignal, aiohttp, torch-geometric\n",
            "Successfully installed aiohappyeyeballs-2.4.3 aiohttp-3.11.2 aiosignal-1.3.1 async-timeout-5.0.1 frozenlist-1.5.0 multidict-6.1.0 propcache-0.2.0 torch-cluster-1.6.3+pt25cpu torch-geometric-2.6.1 torch-scatter-2.1.2+pt25cpu torch-sparse-0.6.18+pt25cpu yarl-1.17.1\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "print(torch.__version__)\n",
        "!pip install torch-scatter torch-sparse torch-cluster torch-geometric -f https://data.pyg.org/whl/torch-{torch.__version__}.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "cac6c9e9-fc1f-4a34-8404-165315a2ebf5",
      "metadata": {
        "id": "cac6c9e9-fc1f-4a34-8404-165315a2ebf5"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch_geometric as pyg\n",
        "from tqdm import tqdm\n",
        "import torch_cluster\n",
        "import sklearn\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "from typing import Optional\n",
        "import numpy as np\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "22cff34d-205e-4ebe-a747-a36fa653895a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "22cff34d-205e-4ebe-a747-a36fa653895a",
        "outputId": "e53f1a5f-cc9a-4558-f771-4d35ec2258af"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "device(type='cpu')"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# find device\n",
        "if torch.cuda.is_available():  # NVIDIA\n",
        "    device = torch.device(\"cuda\")\n",
        "elif torch.backends.mps.is_available():  # apple M1/M2\n",
        "    device = torch.device(\"mps\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "7c275ba2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7c275ba2",
        "outputId": "b8b88009-4ea2-40e8-f985-296902dc7eeb"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.x\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.tx\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.allx\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.y\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.ty\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.ally\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.graph\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.test.index\n",
            "Processing...\n",
            "Done!\n",
            "Downloading https://data.dgl.ai/dataset/ppi.zip\n",
            "Extracting dataset/ppi/ppi.zip\n",
            "Processing...\n",
            "/usr/local/lib/python3.10/dist-packages/networkx/readwrite/json_graph/node_link.py:287: FutureWarning: \n",
            "The default value will be changed to `edges=\"edges\" in NetworkX 3.6.\n",
            "\n",
            "To make this warning go away, explicitly set the edges kwarg, e.g.:\n",
            "\n",
            "  nx.node_link_graph(data, edges=\"links\") to preserve current behavior, or\n",
            "  nx.node_link_graph(data, edges=\"edges\") for forward compatibility.\n",
            "  warnings.warn(\n",
            "Done!\n"
          ]
        }
      ],
      "source": [
        "cora_dataset = pyg.datasets.Planetoid(root=\"./dataset/cora\", name=\"Cora\")\n",
        "cora = cora_dataset[0]\n",
        "ppi_dataset = pyg.datasets.PPI(root=\"./dataset/ppi\")\n",
        "ppi = ppi_dataset[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "2caa4604-9029-4dad-b418-29ce7bd15415",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2caa4604-9029-4dad-b418-29ce7bd15415",
        "outputId": "1c627642-8621-4b62-f017-22613d903551"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Data(x=[2708, 1433], edge_index=[2, 10556], y=[2708], train_mask=[2708], val_mask=[2708], test_mask=[2708])"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "cora"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "058731be-e535-4c13-92e1-42caff76d19f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "058731be-e535-4c13-92e1-42caff76d19f",
        "outputId": "8bf448b8-4c81-4ebd-eb92-67bf94d3e040"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Data(x=[1767, 50], edge_index=[2, 32318], y=[1767, 121])"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ppi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "37ae5939",
      "metadata": {
        "id": "37ae5939"
      },
      "outputs": [],
      "source": [
        "seed = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "2206d9b1",
      "metadata": {
        "id": "2206d9b1"
      },
      "outputs": [],
      "source": [
        "def set_seed(seed: int = 42) -> None:\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    # When running on the CuDNN backend, two further options must be set\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    # Set a fixed value for the hash seed\n",
        "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
        "    print(f\"Random seed set as {seed}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "3841892f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3841892f",
        "outputId": "f35ea5a2-6b33-4c19-b86d-31b360f954b6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Random seed set as 0\n"
          ]
        }
      ],
      "source": [
        "set_seed(seed)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "378d4a84-bda3-484a-87ca-35f95344c594",
      "metadata": {
        "id": "378d4a84-bda3-484a-87ca-35f95344c594"
      },
      "source": [
        "## node2vec embedding training\n",
        "\n",
        "Here the main training and everything on the graph level is happening.\n",
        "\n",
        "It might be a good idea to create a dataset of walks (fixed for the whole training process) first to get the whole training process running before attempting to create a train_loader that on-demand samples those walks on-demand.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "bb739f66-a29d-435a-a2e3-66287f1e4422",
      "metadata": {
        "id": "bb739f66-a29d-435a-a2e3-66287f1e4422"
      },
      "outputs": [],
      "source": [
        "class PQWalkDataset(torch.utils.data.Dataset):\n",
        "    def __init__(\n",
        "        self,\n",
        "        data,\n",
        "        walk_length,\n",
        "        walks_per_node=1,\n",
        "        p=1,\n",
        "        q=1,\n",
        "        num_negative_samples=1,\n",
        "    ):\n",
        "        self.data = data\n",
        "        # check if edge_label_index is present\n",
        "        if hasattr(self.data, \"edge_label_index\"):\n",
        "            self.edge_index = self.data.edge_label_index\n",
        "        else:\n",
        "            self.edge_index = self.data.edge_index\n",
        "        self.walk_length = walk_length - 1\n",
        "        self.walks_per_node = walks_per_node\n",
        "        self.num_nodes = self.data.num_nodes\n",
        "        self.p = p\n",
        "        self.q = q\n",
        "        self.num_negative_samples = num_negative_samples\n",
        "\n",
        "        self._start_nodes = torch.arange(self.num_nodes).repeat(self.walks_per_node)\n",
        "        self._negative_start_nodes = torch.arange(self.num_nodes).repeat(\n",
        "            self.walks_per_node * self.num_negative_samples\n",
        "        )\n",
        "        self._pos_samples = self._get_pos_samples()\n",
        "        self._neg_samples = self._get_neg_samples()\n",
        "\n",
        "    def _get_pos_samples(self):\n",
        "        return torch_cluster.random_walk(\n",
        "            self.edge_index[0],\n",
        "            self.edge_index[1],\n",
        "            start=self._start_nodes,\n",
        "            walk_length=self.walk_length,\n",
        "            p=self.p,\n",
        "            q=self.q,\n",
        "        )\n",
        "\n",
        "    def _get_neg_samples(self):\n",
        "        negative_samples = torch.randint(\n",
        "            0, self.num_nodes, (self._negative_start_nodes.shape[0], self.walk_length)\n",
        "        )\n",
        "        negative_samples = torch.cat(\n",
        "            [self._negative_start_nodes.view(-1, 1), negative_samples], dim=-1\n",
        "        )\n",
        "        return negative_samples\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self._pos_samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        walk = self._pos_samples[idx]\n",
        "        neg_sample = self._neg_samples[idx]\n",
        "        return walk, neg_sample"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "fc22646a",
      "metadata": {
        "id": "fc22646a"
      },
      "outputs": [],
      "source": [
        "# # pqwalkdataset test\n",
        "# cora_pq_dataset = PQWalkDataset(\n",
        "#     data=cora,\n",
        "#     walk_length=4,\n",
        "#     walks_per_node=4,\n",
        "#     num_negative_samples=1,\n",
        "#     seed=42\n",
        "# )\n",
        "\n",
        "# for walk, neg_sample in cora_pq_dataset:\n",
        "#     print(\"Walk shape, Neg sample shape:\", walk.shape, neg_sample.shape)\n",
        "\n",
        "# # test to count walks per node\n",
        "# walk_counts = torch.zeros(cora.num_nodes, dtype=torch.long)\n",
        "# for walk, neg_sample in torch.utils.data.DataLoader(cora_pq_dataset, batch_size=5, num_workers=2, shuffle=True):\n",
        "#     unique, counts = torch.unique(walk[:, 0], return_counts=True)\n",
        "#     for node, count in zip(unique, counts):\n",
        "#         walk_counts[node] += count\n",
        "\n",
        "# print(\"\\nWalk counts per node:\")\n",
        "# for node, count in enumerate(walk_counts):\n",
        "#     print(f\"Node {node}: {count} walks\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "7e5ca47f",
      "metadata": {
        "id": "7e5ca47f"
      },
      "outputs": [],
      "source": [
        "class PQWalkIterableDataset(torch.utils.data.IterableDataset):\n",
        "    def __init__(\n",
        "        self,\n",
        "        data,\n",
        "        walk_length=10,\n",
        "        walks_per_node=10,\n",
        "        p=1,\n",
        "        q=1,\n",
        "        num_negative_samples=1,\n",
        "        batch_size=32,\n",
        "    ):\n",
        "        self.data = data\n",
        "        # check if edge_label_index is present\n",
        "        if hasattr(self.data, \"edge_label_index\"):\n",
        "            self.edge_index = self.data.edge_label_index\n",
        "        else:\n",
        "            self.edge_index = self.data.edge_index\n",
        "        self.walk_length = walk_length - 1\n",
        "        self.walks_per_node = walks_per_node\n",
        "        self.num_nodes = self.data.num_nodes\n",
        "        self.p = p\n",
        "        self.q = q\n",
        "        self.num_negative_samples = num_negative_samples\n",
        "        self.batch_size = min(\n",
        "            batch_size * self.walks_per_node, self.num_nodes * self.walks_per_node\n",
        "        )\n",
        "\n",
        "    def _generate_negative_samples(self, batch_nodes, worker_id):\n",
        "        # Repeat batch nodes for each negative sample\n",
        "        batch = batch_nodes.repeat(self.num_negative_samples)\n",
        "        # print(f\"{worker_id}: Batch shape: {batch.shape}\")\n",
        "        # Generate random walks for negative samples\n",
        "        rw = torch.randint(\n",
        "            self.num_nodes,\n",
        "            (batch.size(0), self.walk_length),\n",
        "            dtype=batch.dtype,\n",
        "            device=batch.device,\n",
        "        )\n",
        "        # Concatenate batch nodes with random walks\n",
        "        rw = torch.cat([batch.view(-1, 1), rw], dim=-1)\n",
        "        return rw\n",
        "\n",
        "    def __iter__(self):\n",
        "        worker_info = torch.utils.data.get_worker_info()\n",
        "        worker_id = 0 if worker_info is None else worker_info.id\n",
        "        num_workers = 1 if worker_info is None else worker_info.num_workers\n",
        "\n",
        "        # print(f\"\\n{worker_id}: Worker {worker_id} calculations:\")\n",
        "\n",
        "        # Calculate nodes per worker\n",
        "        nodes_per_worker = self.num_nodes // num_workers\n",
        "        start_node = worker_id * nodes_per_worker\n",
        "        end_node = (\n",
        "            start_node + nodes_per_worker\n",
        "            if worker_id < num_workers - 1\n",
        "            else self.num_nodes\n",
        "        )\n",
        "        worker_nodes = end_node - start_node\n",
        "\n",
        "        # print(f\"\\n{worker_id}: Handling nodes [{start_node}, {end_node})\")\n",
        "        # print(f\"{worker_id}: Number of nodes for this worker: {worker_nodes}\")\n",
        "        # print(f\"{worker_id}: Walks per node: {self.walks_per_node}\")\n",
        "\n",
        "        # Generate start nodes array that ensures walks_per_node samples for each node\n",
        "        start_nodes = torch.arange(start_node, end_node).repeat_interleave(\n",
        "            self.walks_per_node\n",
        "        )\n",
        "        total_walks = len(start_nodes)\n",
        "        num_batches = (\n",
        "            total_walks + self.batch_size - 1\n",
        "        ) // self.batch_size  # ceiling division\n",
        "\n",
        "        # print(f\"{worker_id}: Total walks to generate: {total_walks}\")\n",
        "        # print(f\"{worker_id}: Batch size: {self.batch_size}\")\n",
        "        # print(f\"{worker_id}: Number of batches: {num_batches}\")\n",
        "\n",
        "        # Shuffle all start nodes\n",
        "        perm = torch.randperm(total_walks)\n",
        "        start_nodes = start_nodes[perm]\n",
        "\n",
        "        # Generate walks in batches\n",
        "        for batch_idx in range(num_batches):\n",
        "            batch_start = batch_idx * self.batch_size\n",
        "            batch_end = min(batch_start + self.batch_size, total_walks)\n",
        "\n",
        "            batch_nodes = start_nodes[batch_start:batch_end]\n",
        "\n",
        "            walks = torch_cluster.random_walk(\n",
        "                self.edge_index[0],\n",
        "                self.edge_index[1],\n",
        "                start=batch_nodes,\n",
        "                walk_length=self.walk_length,\n",
        "                p=self.p,\n",
        "                q=self.q,\n",
        "            )\n",
        "\n",
        "            neg_samples = self._generate_negative_samples(batch_nodes, worker_id)\n",
        "            # no need to return batch_nodes as it is already in the walks\n",
        "            yield walks, neg_samples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "81fd688c-009d-4af5-9c16-f3878701235e",
      "metadata": {
        "id": "81fd688c-009d-4af5-9c16-f3878701235e"
      },
      "outputs": [],
      "source": [
        "# # test walk counts iterable\n",
        "# def test_walk_counts_iterable():\n",
        "#     walks_per_node = 4\n",
        "#     walk_length = 3\n",
        "\n",
        "#     dataset = PQWalkIterableDataset(\n",
        "#         data=cora,\n",
        "#         walk_length=walk_length,\n",
        "#         walks_per_node=walks_per_node,\n",
        "#         num_negative_samples=1,\n",
        "#         seed=42\n",
        "#     )\n",
        "\n",
        "#     dataloader = torch.utils.data.DataLoader(\n",
        "#         dataset,\n",
        "#         batch_size=None,\n",
        "#         num_workers=2\n",
        "#     )\n",
        "\n",
        "#     iter_count = 0\n",
        "#     # Count walks per node\n",
        "#     walk_counts = torch.zeros(cora.num_nodes, dtype=torch.long)\n",
        "#     for pos_sample, neg_sample in dataloader:\n",
        "#         iter_count += 1\n",
        "#         print(f\"\\nPos sample shape, Neg sample shape: {pos_sample.shape}, {neg_sample.shape}\")\n",
        "#         unique, counts = torch.unique(pos_sample[:, 0], return_counts=True)\n",
        "#         for node, count in zip(unique, counts):\n",
        "#             walk_counts[node] += count\n",
        "\n",
        "#     print(f\"\\nWalk counts per node after {iter_count} iterations:\")\n",
        "#     for node, count in enumerate(walk_counts):\n",
        "#         print(f\"Node {node}: {count} walks\")\n",
        "\n",
        "# test_walk_counts_iterable()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "9cda658e",
      "metadata": {
        "id": "9cda658e"
      },
      "outputs": [],
      "source": [
        "# # test walk counts pyg\n",
        "# def test_walk_counts_pyg():\n",
        "#     # Create a small test graph\n",
        "#     walk_length = 3\n",
        "#     walks_per_node = 4\n",
        "#     # Initialize Node2Vec model\n",
        "#     model = Node2Vec(\n",
        "#         edge_index=cora.edge_index,\n",
        "#         embedding_dim=16,\n",
        "#         walk_length=walk_length,\n",
        "#         walks_per_node=walks_per_node,  # Same as walks_per_node\n",
        "#         p=1,\n",
        "#         q=1,\n",
        "#         context_size=walk_length,\n",
        "#     )\n",
        "\n",
        "#     # Create loader\n",
        "#     loader = model.loader(\n",
        "#         batch_size=32,\n",
        "#         shuffle=True,\n",
        "#         num_workers=2\n",
        "#     )\n",
        "#     iter_count = 0\n",
        "\n",
        "#     # Count walks per node\n",
        "#     walk_counts = torch.zeros(cora.num_nodes, dtype=torch.long)\n",
        "#     for pos_sample, neg_sample in loader:\n",
        "#         iter_count += 1\n",
        "#         print(f\"\\nPos sample shape, Neg sample shape: {pos_sample.shape}, {neg_sample.shape}\")\n",
        "#         unique, counts = torch.unique(pos_sample[:, 0], return_counts=True)\n",
        "#         for node, count in zip(unique, counts):\n",
        "#             walk_counts[node] += count\n",
        "\n",
        "#     print(f\"\\nWalk counts per node after {iter_count} iterations:\")\n",
        "#     for node, count in enumerate(walk_counts):\n",
        "#         if count > 0:  # Only print nodes that have walks\n",
        "#             print(f\"Node {node}: {count} walks\")\n",
        "\n",
        "\n",
        "# test_walk_counts_pyg()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "d7c909e8",
      "metadata": {
        "id": "d7c909e8"
      },
      "outputs": [],
      "source": [
        "# edge_index = torch.tensor([\n",
        "#     [0, 1, 1, 2, 1, 3, 3, 4, 4, 2],  # Source nodes\n",
        "#     [1, 0, 2, 1, 3, 1, 4, 3, 2, 4],  # Target nodes\n",
        "# ], dtype=torch.long)\n",
        "\n",
        "# # Create a dummy Data object\n",
        "# class DummyData:\n",
        "#     def __init__(self, edge_index, num_nodes):\n",
        "#         self.edge_index = edge_index\n",
        "#         self.num_nodes = num_nodes\n",
        "\n",
        "# # Create small graph with 5 nodes\n",
        "# small_graph = DummyData(edge_index, num_nodes=5)\n",
        "\n",
        "# # Initialize the dataset with small parameters\n",
        "# dataset = PQWalkIterableDataset(\n",
        "#     data=small_graph,\n",
        "#     walk_length=3,        # Short walks for demonstration\n",
        "#     walks_per_node=2,     # Generate 2 walks per node\n",
        "#     p=1,                  # Return parameter\n",
        "#     q=1,                  # In-out parameter\n",
        "#     num_negative_samples=1,\n",
        "#     batch_size=32,         # Small batch size\n",
        "#     seed=42\n",
        "# )\n",
        "\n",
        "# # Create a dataloader\n",
        "# dataloader = torch.utils.data.DataLoader(\n",
        "#     dataset,\n",
        "#     batch_size=None,  # Batch size is handled by the dataset\n",
        "#     num_workers=2\n",
        "# )\n",
        "\n",
        "# print(\"Graph structure:\")\n",
        "# print(\"Nodes: 0, 1, 2, 3, 4\")\n",
        "# print(\"Edges:\", end=\" \")\n",
        "# for i in range(edge_index.shape[1]):\n",
        "#     print(f\"({edge_index[0][i]}-{edge_index[1][i]})\", end=\" \")\n",
        "# print(\"\\n\")\n",
        "\n",
        "# # Iterate through the batches\n",
        "# for batch_idx, (walks, neg_samples) in enumerate(dataloader):\n",
        "#     print(f\"\\nBatch {batch_idx + 1}:\")\n",
        "#     print(\"\\nPositive walks:\")\n",
        "#     for i, walk in enumerate(walks):\n",
        "#         print(f\"Walk {i + 1}: {walk.tolist()}\")\n",
        "\n",
        "#     print(\"\\nNegative samples:\")\n",
        "#     for i, neg_sample in enumerate(neg_samples):\n",
        "#         print(f\"Negative {i + 1}: {neg_sample.tolist()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "c27881c3",
      "metadata": {
        "id": "c27881c3"
      },
      "outputs": [],
      "source": [
        "# import torch\n",
        "# import torch_geometric as pyg\n",
        "# from torch_geometric.nn import Node2Vec\n",
        "\n",
        "# # Create the same small example graph\n",
        "# # 0 -- 1 -- 2\n",
        "# #      |     |\n",
        "# #      3 -- 4\n",
        "# edge_index = torch.tensor([\n",
        "#     [0, 1, 1, 2, 1, 3, 3, 4, 4, 2],  # Source nodes\n",
        "#     [1, 0, 2, 1, 3, 1, 4, 3, 2, 4],  # Target nodes\n",
        "# ], dtype=torch.long)\n",
        "\n",
        "# # Create a dummy Data object\n",
        "# class DummyData:\n",
        "#     def __init__(self, edge_index, num_nodes):\n",
        "#         self.edge_index = edge_index\n",
        "#         self.num_nodes = num_nodes\n",
        "\n",
        "# # Create small graph with 5 nodes\n",
        "# small_graph = DummyData(edge_index, num_nodes=5)\n",
        "\n",
        "# # Initialize Node2Vec model\n",
        "# model = Node2Vec(\n",
        "#     edge_index=edge_index,\n",
        "#     embedding_dim=16,     # Size of embeddings\n",
        "#     walk_length=3,        # Same as our example\n",
        "#     p=1,                  # Return parameter\n",
        "#     q=1,                  # In-out parameter\n",
        "#     walks_per_node=2,          # Same as our walks_per_node\n",
        "#     context_size=3,\n",
        "# )\n",
        "\n",
        "# # Create loader with the same batch size\n",
        "# loader = model.loader(batch_size=32, shuffle=True)\n",
        "\n",
        "# print(\"Graph structure:\")\n",
        "# print(\"Nodes: 0, 1, 2, 3, 4\")\n",
        "# print(\"Edges:\", end=\" \")\n",
        "# for i in range(edge_index.shape[1]):\n",
        "#     print(f\"({edge_index[0][i]}-{edge_index[1][i]})\", end=\" \")\n",
        "# print(\"\\n\")\n",
        "\n",
        "# # Iterate through the batches\n",
        "# for batch_idx, (pos_rw, neg_rw) in enumerate(loader):\n",
        "#     print(f\"\\nBatch {batch_idx + 1}:\")\n",
        "#     print(\"\\nPositive random walks:\")\n",
        "#     for i, walk in enumerate(pos_rw):\n",
        "#         print(f\"Walk {i + 1}: {walk.tolist()}\")\n",
        "\n",
        "#     print(\"\\nNegative samples:\")\n",
        "#     for i, neg_sample in enumerate(neg_rw):\n",
        "#         print(f\"Negative {i + 1}: {neg_sample.tolist()}\")\n",
        "\n",
        "\n",
        "# # You can also access the generated walks directly\n",
        "# print(\"\\nAll positive random walks:\")\n",
        "# pos_walks = model.pos_sample(batch=torch.arange(small_graph.num_nodes))\n",
        "# print(pos_walks)\n",
        "\n",
        "# print(\"\\nAll negative random walks:\")\n",
        "# neg_walks = model.neg_sample(batch=torch.arange(small_graph.num_nodes))\n",
        "# print(neg_walks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "0a63e7cf-74d9-4d52-b83a-62d98ee54f29",
      "metadata": {
        "id": "0a63e7cf-74d9-4d52-b83a-62d98ee54f29"
      },
      "outputs": [],
      "source": [
        "class Node2Vec(torch.nn.Module):\n",
        "    def __init__(self, embedding_dim: int, num_nodes: int):\n",
        "        super(Node2Vec, self).__init__()\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.num_nodes = num_nodes\n",
        "        self.embedding = torch.nn.Embedding(num_nodes, embedding_dim)\n",
        "        self._EPS = 1e-15\n",
        "\n",
        "    def loss(self, pos_sample, neg_sample):\n",
        "        # print(f\"pos_sample shape: {pos_sample.shape}, neg_sample shape: {neg_sample.shape}\")\n",
        "        assert torch.equal(pos_sample[:, 0], neg_sample[:, 0])\n",
        "        start_nodes = pos_sample[:, 0]\n",
        "        pos_sample_rest = pos_sample[:, 1:].contiguous()\n",
        "        neg_sample_rest = neg_sample[:, 1:].contiguous()\n",
        "        # print(f\" start_nodes shape: {start_nodes.shape}, pos_sample_rest shape: {pos_sample_rest.shape}, neg_sample_rest shape: {neg_sample_rest.shape}\")\n",
        "        # print(\"start node embedding shape:\", self.embedding(start_nodes).shape)\n",
        "        # print(\"rest node embedding shape:\", self.embedding(pos_sample_rest.view(-1)).shape)\n",
        "        h_start = self.embedding(start_nodes).view(\n",
        "            pos_sample.shape[0], 1, self.embedding_dim\n",
        "        )\n",
        "        h_rest = self.embedding(pos_sample_rest).view(\n",
        "            pos_sample.shape[0], -1, self.embedding_dim\n",
        "        )\n",
        "\n",
        "        out = (h_start * h_rest).sum(dim=-1).view(-1)\n",
        "        pos_loss = -torch.log(torch.sigmoid(out) + self._EPS).mean()\n",
        "\n",
        "        h_start = self.embedding(start_nodes).view(\n",
        "            neg_sample.shape[0], 1, self.embedding_dim\n",
        "        )\n",
        "        h_rest = self.embedding(neg_sample_rest.view(-1)).view(\n",
        "            neg_sample.shape[0], -1, self.embedding_dim\n",
        "        )\n",
        "\n",
        "        out = (h_start * h_rest).sum(dim=-1).view(-1)\n",
        "        neg_loss = -torch.log(1 - torch.sigmoid(out) + self._EPS).mean()\n",
        "\n",
        "        return pos_loss + neg_loss\n",
        "\n",
        "    def get_embedding(self):\n",
        "        return self.embedding.weight\n",
        "\n",
        "    def forward(self, pos_sample, neg_sample):\n",
        "        return self.loss(pos_sample, neg_sample)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "647bdf1c-6ebb-462e-82a7-8e57703fa5d2",
      "metadata": {
        "id": "647bdf1c-6ebb-462e-82a7-8e57703fa5d2"
      },
      "outputs": [],
      "source": [
        "def train_node2vec(\n",
        "    data: pyg.data.Data,\n",
        "    walk_length: int,\n",
        "    walks_per_node: int,\n",
        "    embedding_dim: int,\n",
        "    p: float = 1,\n",
        "    q: float = 1,\n",
        "    num_negative_samples: int = 1,\n",
        "    batch_size: int = 32,\n",
        "    lr: float = 0.01,\n",
        "    num_epochs: int = 200,\n",
        "    num_workers: int = 4,\n",
        "    # scheduler_type: str = 'cosine'  # Options: 'step', 'cosine', 'reduce_on_plateau'\n",
        "):\n",
        "    dataset = PQWalkIterableDataset(\n",
        "        data=data,\n",
        "        walk_length=walk_length,\n",
        "        walks_per_node=walks_per_node,\n",
        "        num_negative_samples=num_negative_samples,\n",
        "        batch_size=batch_size,\n",
        "        p=p,\n",
        "        q=q,\n",
        "    )\n",
        "\n",
        "    dataloader = torch.utils.data.DataLoader(\n",
        "        dataset,\n",
        "        batch_size=None,\n",
        "        num_workers=num_workers,\n",
        "        pin_memory=True if device.type == \"cuda\" else False,\n",
        "    )\n",
        "\n",
        "    model = Node2Vec(embedding_dim, data.num_nodes)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "    # Initialize scheduler based on type\n",
        "    # if scheduler_type == 'step':\n",
        "    #     scheduler = torch.optim.lr_scheduler.StepLR(\n",
        "    #         optimizer,\n",
        "    #         step_size=30,  # Decrease LR every 30 epochs\n",
        "    #         gamma=0.1      # Multiply LR by 0.1\n",
        "    #     )\n",
        "    # elif scheduler_type == 'cosine':\n",
        "    #     scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
        "    #         optimizer,\n",
        "    #         T_max=num_epochs,  # Full period of cosine annealing\n",
        "    #         eta_min=1e-5      # Minimum learning rate\n",
        "    #     )\n",
        "    # elif scheduler_type == 'reduce_on_plateau':\n",
        "    #     scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "    #         optimizer,\n",
        "    #         mode='min',     # Reduce LR when metric stops decreasing\n",
        "    #         factor=0.1,     # Multiply LR by 0.1\n",
        "    #         patience=10,    # Number of epochs to wait before reducing LR\n",
        "    #         min_lr=1e-6     # Minimum LR\n",
        "    #     )\n",
        "    # else:\n",
        "    #     scheduler = None\n",
        "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
        "        optimizer,\n",
        "        T_max=num_epochs,  # Full period of cosine annealing\n",
        "        eta_min=1e-5,  # Minimum learning rate\n",
        "    )\n",
        "    model = model.to(device)\n",
        "    model.train()\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        total_loss = 0\n",
        "        num_batches = 0\n",
        "\n",
        "        for pos_sample, neg_sample in dataloader:\n",
        "            pos_sample = pos_sample.to(device)\n",
        "            neg_sample = neg_sample.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            loss = model(pos_sample, neg_sample)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            num_batches += 1\n",
        "\n",
        "        # Calculate average loss for this epoch\n",
        "        avg_loss = total_loss / num_batches\n",
        "        scheduler.step()\n",
        "        if (epoch + 1) % 10 == 0:\n",
        "            current_lr = optimizer.param_groups[0][\"lr\"]\n",
        "            print(f\"Epoch {epoch+1:02d}, Loss: {avg_loss:.4f}, LR: {current_lr:.6f}\")\n",
        "\n",
        "    return model.get_embedding()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "ddcef892",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ddcef892",
        "outputId": "6cd1daad-fb6b-4dff-ad8c-56bd67701507"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 10, Loss: 1.2166, LR: 0.009756\n",
            "Epoch 20, Loss: 1.0817, LR: 0.009046\n",
            "Epoch 30, Loss: 1.0749, LR: 0.007941\n",
            "Epoch 40, Loss: 1.0713, LR: 0.006549\n",
            "Epoch 50, Loss: 1.0693, LR: 0.005005\n",
            "Epoch 60, Loss: 1.0651, LR: 0.003461\n",
            "Epoch 70, Loss: 1.0624, LR: 0.002069\n",
            "Epoch 80, Loss: 1.0591, LR: 0.000964\n",
            "Epoch 90, Loss: 1.0582, LR: 0.000254\n",
            "Epoch 100, Loss: 1.0585, LR: 0.000010\n"
          ]
        }
      ],
      "source": [
        "node2vec_embeddings = train_node2vec(\n",
        "    cora, 100, 10, 128, p=0.5, q=2.0, lr=0.01, num_epochs=100\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1f6e8d3b",
      "metadata": {
        "id": "1f6e8d3b"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "50c83c75",
      "metadata": {
        "id": "50c83c75"
      },
      "outputs": [],
      "source": [
        "embedding_dim = node2vec_embeddings.shape[1]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9bfe71e5-f122-46ff-abdd-7b0fd15d56fc",
      "metadata": {
        "id": "9bfe71e5-f122-46ff-abdd-7b0fd15d56fc"
      },
      "source": [
        "## Node classification performance\n",
        "\n",
        "just a small MLP or even linear layer on the embeddings to predict node classes. Accuracy should be above 60%. Please compare your results to those you achieved with GNNs.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "3ca95d6c-ed3f-49eb-b733-17a9035db399",
      "metadata": {
        "id": "3ca95d6c-ed3f-49eb-b733-17a9035db399"
      },
      "outputs": [],
      "source": [
        "# as the simple MLP is pretty straightforward\n",
        "model = torch.nn.Sequential(\n",
        "    torch.nn.Linear(embedding_dim, 256),  # Input layer\n",
        "    torch.nn.ReLU(),\n",
        "    torch.nn.Linear(256, 128),  # Hidden layer 2\n",
        "    torch.nn.ReLU(),\n",
        "    torch.nn.Linear(128, cora_dataset.num_classes),  # Output layer\n",
        ")\n",
        "model = model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "2f648c85-bf32-42f0-94f0-19e052f70325",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2f648c85-bf32-42f0-94f0-19e052f70325",
        "outputId": "145d5c61-77d0-4c8a-b752-86e0215063aa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 10, Loss: 6.090e-02\n",
            "Epoch 20, Loss: 3.035e-04\n",
            "Epoch 30, Loss: 1.436e-05\n",
            "Epoch 40, Loss: 2.259e-06\n",
            "Epoch 50, Loss: 1.217e-06\n",
            "Epoch 60, Loss: 6.744e-07\n",
            "Epoch 70, Loss: 4.334e-07\n",
            "Epoch 80, Loss: 3.270e-07\n",
            "Epoch 90, Loss: 2.733e-07\n",
            "Epoch 100, Loss: 2.401e-07\n",
            "node classification accuracy for cora: 0.68 (train: 1.00, val: 0.69)\n"
          ]
        }
      ],
      "source": [
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)  # define an optimizer\n",
        "criterion = torch.nn.CrossEntropyLoss()  # define loss function\n",
        "\n",
        "node2vec_embeddings = node2vec_embeddings.to(device)\n",
        "cora = cora.to(device)\n",
        "\n",
        "for epoch in range(100):  # 100 epochs\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    out = model(node2vec_embeddings[cora.train_mask])  # forward pass\n",
        "    loss = criterion(out, cora.y[cora.train_mask])\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # print out loss info\n",
        "    if (epoch + 1) % 10 == 0:\n",
        "        print(f\"Epoch {epoch+1}, Loss: {loss.item():.3e}\")\n",
        "\n",
        "\n",
        "def get_accuracy(model, embeddings, y, mask):\n",
        "    out = model(embeddings[mask])\n",
        "    pred = out.argmax(dim=1)\n",
        "    acc = accuracy_score(y[mask].cpu().numpy(), pred.cpu().detach().numpy())\n",
        "    return acc\n",
        "\n",
        "\n",
        "train_acc = get_accuracy(model, node2vec_embeddings, cora.y, cora.train_mask)\n",
        "val_acc = get_accuracy(model, node2vec_embeddings, cora.y, cora.val_mask)\n",
        "test_acc = get_accuracy(model, node2vec_embeddings, cora.y, cora.test_mask)\n",
        "\n",
        "print(\n",
        "    f\"node classification accuracy for cora: {test_acc:.2f} (train: {train_acc:.2f}, val: {val_acc:.2f})\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "f7712ab1-4753-4715-abaf-33b666680535",
      "metadata": {
        "id": "f7712ab1-4753-4715-abaf-33b666680535"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "c378a139-b47e-49a1-a65b-90e7a96ca165",
      "metadata": {
        "id": "c378a139-b47e-49a1-a65b-90e7a96ca165"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "83edad57-6fa5-466a-b3d7-56244a79f410",
      "metadata": {
        "id": "83edad57-6fa5-466a-b3d7-56244a79f410"
      },
      "source": [
        "## link prediction on trained embeddings\n",
        "\n",
        "this should only train simple MLPs.\n",
        "\n",
        "Note: for link prediction to be worthwhile, one needs to train the embeddings on a subset of the graph (less edges, same nodes) instead of the whole graph.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "290ffae9-7872-441a-9e63-385619295400",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "290ffae9-7872-441a-9e63-385619295400",
        "outputId": "6ff9fa54-8b80-486b-c6f8-f3c23fbff430"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Data(x=[2708, 1433], edge_index=[2, 7392], y=[2708], train_mask=[2708], val_mask=[2708], test_mask=[2708], edge_label=[7392], edge_label_index=[2, 7392])"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# for link prediction, do something like the following\n",
        "link_splitter = pyg.transforms.RandomLinkSplit(is_undirected=True)\n",
        "train_data, val_data, test_data = link_splitter(cora)\n",
        "train_data\n",
        "# the positive and negative edges are in \"edge_label_index\" with \"edge_label\"\n",
        "# indicating whether an edge is a true edge or not."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "4e295578-2a40-4eba-a580-8cc7502492f5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4e295578-2a40-4eba-a580-8cc7502492f5",
        "outputId": "5166eccb-2088-4a2b-cad7-f6a657e57d0a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[ 141, 1221,  567,  ..., 1705, 1668, 1989],\n",
              "        [2034, 1577, 1262,  ...,  306, 1527, 1986]])"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test_data.edge_index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "1ef3ab50",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ef3ab50",
        "outputId": "db6aafa6-4c6d-48fe-c262-a0a1c8a12c07"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[ 492,  415, 1334,  ..., 2255, 1754, 2356],\n",
              "        [2678, 1644, 1941,  ...,  450, 2102, 1942]])"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test_data.edge_label_index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "fc451bdf-859d-4592-9708-43c93aa8fbb1",
      "metadata": {
        "id": "fc451bdf-859d-4592-9708-43c93aa8fbb1"
      },
      "outputs": [],
      "source": [
        "# retrain node2vec on train_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "cc7cdc42-69e8-4320-b5df-c044ea48b52f",
      "metadata": {
        "id": "cc7cdc42-69e8-4320-b5df-c044ea48b52f"
      },
      "outputs": [],
      "source": [
        "def calculate_mrr(embeddings, pos_edge_index, all_edges, mode=\"filtered\", k=None):\n",
        "    \"\"\"\n",
        "    Calculate MRR with different filtering modes\n",
        "\n",
        "    mode: 'raw', 'filtered', or 'more-filtered'\n",
        "    k: if not None, only consider the top k ranks\n",
        "    \"\"\"\n",
        "    mrr_list = []\n",
        "    num_nodes = embeddings.size(0)\n",
        "\n",
        "    # Convert existing edges to set for faster lookup\n",
        "    existing_edges = set(map(tuple, all_edges.t().tolist()))\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i in range(pos_edge_index.size(1)):\n",
        "            source = pos_edge_index[0, i]\n",
        "            target = pos_edge_index[1, i]\n",
        "\n",
        "            source_emb = embeddings[source].unsqueeze(0)\n",
        "            all_scores = torch.mm(source_emb, embeddings.t()).squeeze()\n",
        "\n",
        "            # Different filtering modes\n",
        "            if mode == \"raw\":\n",
        "                # No filtering - includes all edges (not recommended)\n",
        "                pass\n",
        "\n",
        "            elif mode == \"filtered\":\n",
        "                # Filter out existing edges except the target\n",
        "                for j in range(num_nodes):\n",
        "                    if (source.item(), j) in existing_edges and j != target.item():\n",
        "                        all_scores[j] = float(\"-inf\")\n",
        "\n",
        "            elif mode == \"more-filtered\":\n",
        "                # Filter existing edges and self-loops\n",
        "                for j in range(num_nodes):\n",
        "                    if (\n",
        "                        (source.item(), j) in existing_edges and j != target.item()\n",
        "                    ) or j == source.item():\n",
        "                        all_scores[j] = float(\"-inf\")\n",
        "\n",
        "            sorted_indices = torch.argsort(all_scores, descending=True)\n",
        "            rank = (sorted_indices == target).nonzero().item() + 1\n",
        "\n",
        "            if k is not None and rank > k:\n",
        "                mrr_list.append(0)\n",
        "            else:\n",
        "                mrr_list.append(1.0 / rank)\n",
        "\n",
        "    return sum(mrr_list) / len(mrr_list)\n",
        "\n",
        "\n",
        "def get_link_labels(pos_edge_index, neg_edge_index):\n",
        "    \"\"\"\n",
        "    Creates labels for positive and negative edges\n",
        "    \"\"\"\n",
        "    num_links = pos_edge_index.size(1) + neg_edge_index.size(1)\n",
        "    link_labels = torch.zeros(num_links, dtype=torch.float)\n",
        "    link_labels[: pos_edge_index.size(1)] = 1.0\n",
        "    return link_labels\n",
        "\n",
        "\n",
        "def get_edge_embeddings(embeddings, edge_index, merge_method: str = \"average\"):\n",
        "    \"\"\"\n",
        "    Combine node embeddings to create edge embeddings\n",
        "    \"\"\"\n",
        "    # Get node embeddings for both source and target nodes\n",
        "    src_embeddings = embeddings[edge_index[0]]\n",
        "    dst_embeddings = embeddings[edge_index[1]]\n",
        "\n",
        "    # Different ways to combine the embeddings\n",
        "    if merge_method == \"hadamard\":\n",
        "        edge_embedding = src_embeddings * dst_embeddings\n",
        "    elif merge_method == \"average\":\n",
        "        edge_embedding = (src_embeddings + dst_embeddings) / 2\n",
        "    elif merge_method == \"l1\":\n",
        "        edge_embedding = torch.abs(src_embeddings - dst_embeddings)\n",
        "    elif merge_method == \"l2\":\n",
        "        edge_embedding = torch.norm(src_embeddings - dst_embeddings, dim=1)\n",
        "    return edge_embedding\n",
        "\n",
        "\n",
        "def evaluate_link_prediction(\n",
        "    embeddings,\n",
        "    edge_classifier,\n",
        "    pos_edge_index,\n",
        "    neg_edge_index,\n",
        "    merge_method: str = \"average\",\n",
        "):\n",
        "    \"\"\"\n",
        "    Evaluate link prediction performance\n",
        "    \"\"\"\n",
        "    # Get edge embeddings\n",
        "    with torch.no_grad():  # Don't track gradients for embeddings\n",
        "        pos_edge_embeddings = get_edge_embeddings(\n",
        "            embeddings, pos_edge_index, merge_method\n",
        "        )\n",
        "        neg_edge_embeddings = get_edge_embeddings(\n",
        "            embeddings, neg_edge_index, merge_method\n",
        "        )\n",
        "\n",
        "        # Combine positive and negative edge embeddings\n",
        "        edge_embeddings = torch.cat([pos_edge_embeddings, neg_edge_embeddings], dim=0)\n",
        "\n",
        "        # Create labels\n",
        "        labels = get_link_labels(pos_edge_index, neg_edge_index)\n",
        "\n",
        "    # Evaluate\n",
        "    edge_classifier.eval()\n",
        "    with torch.no_grad():\n",
        "        pred = edge_classifier(edge_embeddings).squeeze()\n",
        "        auc_score = sklearn.metrics.roc_auc_score(labels.cpu(), pred.cpu())\n",
        "        ap_score = sklearn.metrics.average_precision_score(labels.cpu(), pred.cpu())\n",
        "    all_edges = torch.cat([pos_edge_index, neg_edge_index], dim=1)\n",
        "    mrr = calculate_mrr(embeddings, pos_edge_index, all_edges)\n",
        "    return auc_score, ap_score, mrr\n",
        "\n",
        "\n",
        "def train_and_evaluate_link_prediction(\n",
        "    data,\n",
        "    walk_length: int = 100,\n",
        "    walks_per_node: int = 10,\n",
        "    embedding_dim: int = 128,\n",
        "    p: float = 1,\n",
        "    q: float = 1,\n",
        "    lr: float = 0.01,\n",
        "    num_epochs: int = 100,\n",
        "    merge_method: str = \"average\",\n",
        "):\n",
        "    link_splitter = pyg.transforms.RandomLinkSplit(is_undirected=True)\n",
        "    train_data, val_data, test_data = link_splitter(data)\n",
        "    # print(hasattr(train_data, 'edge_label_index'))\n",
        "\n",
        "    embeddings = train_node2vec(\n",
        "        train_data,\n",
        "        walk_length,\n",
        "        walks_per_node,\n",
        "        embedding_dim,\n",
        "        p,\n",
        "        q,\n",
        "        lr=lr,\n",
        "        num_epochs=num_epochs,\n",
        "    )\n",
        "    embeddings = embeddings.detach()\n",
        "\n",
        "    # Train a simple classifier\n",
        "    edge_classifier = torch.nn.Sequential(\n",
        "        torch.nn.Linear(embeddings.shape[1], 64),\n",
        "        torch.nn.ReLU(),\n",
        "        torch.nn.Linear(64, 1),\n",
        "        torch.nn.Sigmoid(),\n",
        "    ).to(device)\n",
        "\n",
        "    # Train the classifier\n",
        "    optimizer = torch.optim.Adam(edge_classifier.parameters(), lr=0.01)\n",
        "    criterion = torch.nn.BCELoss()\n",
        "\n",
        "    pos_edge_embeddings = get_edge_embeddings(\n",
        "        embeddings,\n",
        "        train_data.edge_label_index[:, train_data.edge_label == 1],\n",
        "        merge_method,\n",
        "    )\n",
        "    neg_edge_embeddings = get_edge_embeddings(\n",
        "        embeddings,\n",
        "        train_data.edge_label_index[:, train_data.edge_label == 0],\n",
        "        merge_method,\n",
        "    )\n",
        "\n",
        "    # Combine positive and negative edge embeddings\n",
        "    edge_embeddings = torch.cat([pos_edge_embeddings, neg_edge_embeddings], dim=0)\n",
        "\n",
        "    # Create labels\n",
        "    labels = get_link_labels(\n",
        "        train_data.edge_label_index[:, train_data.edge_label == 1],\n",
        "        train_data.edge_label_index[:, train_data.edge_label == 0],\n",
        "    )\n",
        "\n",
        "    edge_embeddings = edge_embeddings.to(device)\n",
        "    labels = labels.to(device)\n",
        "\n",
        "    # Training loop\n",
        "    edge_classifier.train()\n",
        "    for epoch in range(100):\n",
        "        optimizer.zero_grad()\n",
        "        out = edge_classifier(edge_embeddings).squeeze()\n",
        "        loss = criterion(out, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    # Evaluate on validation set\n",
        "    val_auc, val_ap, val_mrr = evaluate_link_prediction(\n",
        "        embeddings,\n",
        "        edge_classifier,\n",
        "        val_data.edge_label_index[:, val_data.edge_label == 1],\n",
        "        val_data.edge_label_index[:, val_data.edge_label == 0],\n",
        "        merge_method,\n",
        "    )\n",
        "\n",
        "    # Evaluate on test set\n",
        "    test_auc, test_ap, test_mrr = evaluate_link_prediction(\n",
        "        embeddings,\n",
        "        edge_classifier,\n",
        "        test_data.edge_label_index[:, test_data.edge_label == 1],\n",
        "        test_data.edge_label_index[:, test_data.edge_label == 0],\n",
        "        merge_method,\n",
        "    )\n",
        "\n",
        "    print(f\"Validation - AUC: {val_auc:.4f}, AP: {val_ap:.4f}, MRR: {val_mrr:.4f}\")\n",
        "    print(f\"Test - AUC: {test_auc:.4f}, AP: {test_ap:.4f}, MRR: {test_mrr:.4f}\")\n",
        "\n",
        "    return embeddings, (val_auc, val_ap, val_mrr), (test_auc, test_ap, test_mrr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "id": "e9f11bab-e219-4434-9e2d-16b2e3c702c9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e9f11bab-e219-4434-9e2d-16b2e3c702c9",
        "outputId": "6b5d6e63-a769-48be-f7cc-b73c072d3653"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Link prediction on Cora:\n",
            "Epoch 10, Loss: 2.7561, LR: 0.009046\n",
            "Epoch 20, Loss: 1.4622, LR: 0.006549\n",
            "Epoch 30, Loss: 1.1936, LR: 0.003461\n",
            "Epoch 40, Loss: 1.1203, LR: 0.000964\n",
            "Epoch 50, Loss: 1.1124, LR: 0.000010\n",
            "Validation - AUC: 0.5869, AP: 0.5887, MRR: 0.0062\n",
            "Test - AUC: 0.5911, AP: 0.6070, MRR: 0.0067\n",
            "Cora embeddings shape: torch.Size([2708, 128])\n",
            "Cora validation results: AUC: 0.5869, AP: 0.5887, MRR: 0.0062\n",
            "Cora test results: AUC: 0.5911, AP: 0.6070, MRR: 0.0067\n"
          ]
        }
      ],
      "source": [
        "# use those (new) embeddings for link prediction\n",
        "print(\"Link prediction on Cora:\")\n",
        "cora_results = train_and_evaluate_link_prediction(\n",
        "    cora, 100, 10, 128, p=0.5, q=2.0, lr=0.01, num_epochs=50, merge_method=\"average\"\n",
        ")\n",
        "cora_embeddings, cora_val_results, cora_test_results = cora_results\n",
        "print(f\"Cora embeddings shape: {cora_embeddings.shape}\")\n",
        "print(\n",
        "    f\"Cora validation results: AUC: {cora_val_results[0]:.4f}, AP: {cora_val_results[1]:.4f}, MRR: {cora_val_results[2]:.4f}\"\n",
        ")\n",
        "print(\n",
        "    f\"Cora test results: AUC: {cora_test_results[0]:.4f}, AP: {cora_test_results[1]:.4f}, MRR: {cora_test_results[2]:.4f}\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "id": "4a632698-60fd-40d6-b3c3-d924402f24e5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4a632698-60fd-40d6-b3c3-d924402f24e5",
        "outputId": "e80a4ef1-6c84-44b7-817b-26871402286b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Link prediction on PPI:\n",
            "Epoch 10, Loss: 1.4844, LR: 0.009046\n",
            "Epoch 20, Loss: 1.3694, LR: 0.006549\n",
            "Epoch 30, Loss: 1.3587, LR: 0.003461\n",
            "Epoch 40, Loss: 1.3553, LR: 0.000964\n",
            "Epoch 50, Loss: 1.3552, LR: 0.000010\n",
            "Validation - AUC: 0.8209, AP: 0.8164, MRR: 0.0252\n",
            "Test - AUC: 0.8197, AP: 0.8161, MRR: 0.0231\n",
            "PPI embeddings shape: torch.Size([1767, 128])\n",
            "PPI validation results: AUC: 0.8209, AP: 0.8164, MRR: 0.0252\n",
            "PPI test results: AUC: 0.8197, AP: 0.8161, MRR: 0.0231\n"
          ]
        }
      ],
      "source": [
        "print(\"Link prediction on PPI:\")\n",
        "ppi_results = train_and_evaluate_link_prediction(\n",
        "    ppi, 100, 10, 128, p=0.5, q=2.0, lr=0.01, num_epochs=50, merge_method=\"average\"\n",
        ")\n",
        "ppi_embeddings, ppi_val_results, ppi_test_results = ppi_results\n",
        "print(f\"PPI embeddings shape: {ppi_embeddings.shape}\")\n",
        "print(\n",
        "    f\"PPI validation results: AUC: {ppi_val_results[0]:.4f}, AP: {ppi_val_results[1]:.4f}, MRR: {ppi_val_results[2]:.4f}\"\n",
        ")\n",
        "print(\n",
        "    f\"PPI test results: AUC: {ppi_test_results[0]:.4f}, AP: {ppi_test_results[1]:.4f}, MRR: {ppi_test_results[2]:.4f}\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "c840653f-8348-4792-86fd-59679ee523bf",
      "metadata": {
        "id": "c840653f-8348-4792-86fd-59679ee523bf"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "96f0649e-7dbc-441b-b019-1f7239a5c5c1",
      "metadata": {
        "id": "96f0649e-7dbc-441b-b019-1f7239a5c5c1"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V28",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.15"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
