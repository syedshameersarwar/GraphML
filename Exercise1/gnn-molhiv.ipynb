{"metadata":{"accelerator":"TPU","colab":{"gpuType":"V28","provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Dense GNN implementation\n\nIn this exercise we are implementing a GNN from scratch using dense matrices.\nNote that as the memory requirement of a dense matrix scales quadratically with the number of nodes in a graph, this limits us to datasets with only small graphs.\n\nWe will use the following dataset molHIV.\n\nFor the network we need a message-passing layer and pooling function.\n\n1. Describe the datasets in your own words. Also talk about its features and statistical properties of the graphs and labels.\n1. Implement the class GCNLayer to perform one round of message passing. You may use any variant of message passing here.\n1. Implement a pooling layer like MeanPooling or SumPooling (or both).\n1. Implement a one-hot-encoding of the atom type (this will positively affect classification performance)\n1. Implement the model class GraphGCN that builds upon your GCNLayer and Pooling layer.\n1. Create and train a GraphGCN model on MolHIV. As MOlHIV is highly imbalanced, it will make sense to adapt class weights in your loss function.\n\nFor the dataset molHIV we aim to reach something like 0.64 ROC (or higher). Note that for me the training was quite unstable, so several runs got stuck at 0.5.\n\nNote: In this exercise, we use PyG only for utilities and not to build models. Feel free to edit/ignore any of the provided code as you see fit.","metadata":{"id":"fb719674-c334-440d-af62-1aed8c57f1ec"}},{"cell_type":"code","source":"!pip install ogb torch_geometric","metadata":{"id":"Z_8-YE5Y7Ndg","outputId":"878cfe63-bce0-4a4a-ce09-89b0850d6445","execution":{"iopub.status.busy":"2024-10-28T21:34:28.697987Z","iopub.execute_input":"2024-10-28T21:34:28.698711Z","iopub.status.idle":"2024-10-28T21:34:43.476091Z","shell.execute_reply.started":"2024-10-28T21:34:28.698648Z","shell.execute_reply":"2024-10-28T21:34:43.475093Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch_geometric as pyg\nimport numpy as np\nfrom ogb.graphproppred import PygGraphPropPredDataset, Evaluator\nimport matplotlib.pyplot as plt\nimport os\n\nfrom tqdm import tqdm","metadata":{"id":"78608f62-c77d-4f16-a924-50843e5bcc85","execution":{"iopub.status.busy":"2024-10-28T21:34:43.477907Z","iopub.execute_input":"2024-10-28T21:34:43.478216Z","iopub.status.idle":"2024-10-28T21:34:50.530468Z","shell.execute_reply.started":"2024-10-28T21:34:43.478181Z","shell.execute_reply":"2024-10-28T21:34:50.529410Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# find device\nif torch.cuda.is_available():  # NVIDIA\n    device = torch.device(\"cuda\")\nelif torch.backends.mps.is_available():  # apple silicon\n    device = torch.device(\"mps\")\nelse:\n    device = torch.device(\"cpu\")  # fallback\ndevice","metadata":{"id":"906e13d0-cf25-450f-948c-c8f3fa214850","outputId":"660ceaad-dd61-4b44-ace3-229aceccd773","execution":{"iopub.status.busy":"2024-10-28T21:34:50.532268Z","iopub.execute_input":"2024-10-28T21:34:50.532785Z","iopub.status.idle":"2024-10-28T21:34:50.541644Z","shell.execute_reply.started":"2024-10-28T21:34:50.532742Z","shell.execute_reply":"2024-10-28T21:34:50.540542Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def set_seed(seed: int = 42) -> None:\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    # When running on the CuDNN backend, two further options must be set\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n    # Set a fixed value for the hash seed\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    print(f\"Random seed set as {seed}\")\n\nset_seed()","metadata":{"id":"aC14WIdxhRNA","outputId":"e8cfa4f7-9e50-4bd8-e9dd-31f01e5ed5f1","execution":{"iopub.status.busy":"2024-10-28T21:34:50.544822Z","iopub.execute_input":"2024-10-28T21:34:50.545122Z","iopub.status.idle":"2024-10-28T21:34:50.588870Z","shell.execute_reply.started":"2024-10-28T21:34:50.545089Z","shell.execute_reply":"2024-10-28T21:34:50.587885Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class GCNLayer(torch.nn.Module):\n    def __init__(\n        self,\n        in_features: int,\n        out_features: int,\n        activation=torch.nn.functional.relu,\n        skip_connection=False,\n    ):\n        super(GCNLayer, self).__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        self.activation = activation\n        self.skip_connection = skip_connection\n        self.weight = torch.nn.Parameter(torch.FloatTensor(in_features, out_features))\n        torch.nn.init.kaiming_normal_(self.weight)\n\n    def forward(self, H: torch.Tensor, adj: torch.Tensor):\n        h_x = torch.bmm(adj, torch.matmul(H, self.weight))\n        if self.skip_connection:\n            return self.activation(h_x + H)\n        return self.activation(h_x)","metadata":{"id":"76a3a3e8-b9cb-42d7-be0f-8c41a1304a8d","execution":{"iopub.status.busy":"2024-10-28T21:34:50.590230Z","iopub.execute_input":"2024-10-28T21:34:50.590508Z","iopub.status.idle":"2024-10-28T21:34:50.599263Z","shell.execute_reply.started":"2024-10-28T21:34:50.590477Z","shell.execute_reply":"2024-10-28T21:34:50.598244Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class MeanPooling(torch.nn.Module):\n    def __init__(self, dim: int | tuple[int, ...]):\n        super(MeanPooling, self).__init__()\n        self.dim = dim\n\n    def forward(self, H: torch.Tensor):\n        return H.mean(dim=self.dim)","metadata":{"id":"f07521e9","execution":{"iopub.status.busy":"2024-10-28T21:34:50.600638Z","iopub.execute_input":"2024-10-28T21:34:50.600963Z","iopub.status.idle":"2024-10-28T21:34:50.610370Z","shell.execute_reply.started":"2024-10-28T21:34:50.600931Z","shell.execute_reply":"2024-10-28T21:34:50.609509Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class SumPooling(torch.nn.Module):\n    def __init__(self, dim: int | tuple[int, ...]):\n        super(SumPooling, self).__init__()\n        self.dim = dim\n\n    def forward(self, H: torch.Tensor):\n        return H.sum(dim=self.dim)","metadata":{"id":"0bc412b3-5648-4f1f-88a0-f5a18a27a419","execution":{"iopub.status.busy":"2024-10-28T21:34:50.611672Z","iopub.execute_input":"2024-10-28T21:34:50.611989Z","iopub.status.idle":"2024-10-28T21:34:50.622252Z","shell.execute_reply.started":"2024-10-28T21:34:50.611957Z","shell.execute_reply":"2024-10-28T21:34:50.621452Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class GraphGCN(torch.nn.Module):\n    def __init__(\n        self,\n        num_layers: int,\n        in_features: int,\n        hidden_features: int,\n        out_features: int,\n        pooling: MeanPooling | SumPooling,\n        activation=torch.nn.functional.relu,\n        skip_connection: bool = False,\n        mlp_dropout_rate=0.1,\n    ):\n        super(GraphGCN, self).__init__()\n        self.pooling = pooling\n        self.activation = activation\n        self.skip_connection = skip_connection\n        self.layers = torch.nn.ModuleList(\n            [\n                GCNLayer(\n                    in_features=in_features if i == 0 else hidden_features,\n                    out_features=hidden_features,\n                    activation=activation,\n                    skip_connection=skip_connection if i != 0 else False,\n                )\n                for i in range(num_layers)\n            ]\n        )\n        self.mlp = torch.nn.Sequential(\n            torch.nn.Linear(hidden_features, hidden_features),\n            torch.nn.ReLU(),\n            torch.nn.Dropout(mlp_dropout_rate),\n            torch.nn.Linear(hidden_features, out_features),\n        )\n\n    def forward(self, H_in: torch.Tensor, adj: torch.Tensor):\n        H = H_in\n        for i in range(len(self.layers)):\n            H = self.layers[i](H, adj)\n        H = self.pooling(H)\n        return self.mlp(H)","metadata":{"id":"add7fe46-769b-4d5c-8c9a-115c877132e2","execution":{"iopub.status.busy":"2024-10-28T21:34:50.623278Z","iopub.execute_input":"2024-10-28T21:34:50.623582Z","iopub.status.idle":"2024-10-28T21:34:50.636495Z","shell.execute_reply.started":"2024-10-28T21:34:50.623551Z","shell.execute_reply":"2024-10-28T21:34:50.635793Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## MolHIV\n\nPytorch Geometric stores its graphs in a sparse format using the variable edge_index.\nWe will thus need to create our own (torch) dataloader and extract the graphs into dense adjacency matrices.\n\nIn terms of model accuracy, it really helped me to add an \"Atom encoding\", i.e. a one-hot-encoding of the atoms instead of just having the atomic numbers appear in the first column of the node features.","metadata":{"id":"4c9a35b2"}},{"cell_type":"code","source":"class GraphDataSetVectorized(torch.utils.data.Dataset):\n    def __init__(self, dataset):\n        self._dataset = dataset\n        self._largest_graph_size = int(dataset.get_summary().num_nodes.max)\n        self.targets = self._dataset.data.y\n        self._max_num_atoms = 119\n        self.atoms_to_index = {i: i - 1 for i in range(1, self._max_num_atoms + 1)}\n\n    def __len__(self):\n        return len(self.targets)\n\n    def __getitem__(self, idx):\n        graph = self._dataset[idx]\n\n        # adjacency matrix\n        A = torch.zeros((self._largest_graph_size, self._largest_graph_size))\n        # symmetric\n        A[graph.edge_index[0], graph.edge_index[1]] = 1\n        # self loop\n        A = A + torch.eye(self._largest_graph_size)\n        # Degree matrix\n        D = torch.diag(torch.sum(A, axis=1))\n        # Normalized adjacency matrix\n        d_inv_sqrt = torch.pow(D, -0.5)\n        d_inv_sqrt[torch.isinf(d_inv_sqrt)] = 0.0\n        A_normalized = d_inv_sqrt @ A @ d_inv_sqrt\n\n        # node features\n        H = torch.zeros((self._largest_graph_size, graph.x.shape[1]))\n        H[: graph.x.shape[0]] = graph.x\n        # one-hot encoding of atomic number\n        atomic_no_feature = H[:, 0].long()\n        atomic_no_feature = torch.nn.functional.one_hot(\n            atomic_no_feature, num_classes=self._max_num_atoms\n        )\n        H = torch.cat([atomic_no_feature, H[:, 1:]], dim=-1)\n\n        # target\n        target = graph.y\n        return A_normalized, H, target\n\n    def num_features(self):\n        return (\n            self._dataset.num_features + self._max_num_atoms - 1\n        )  # atomic number one-hot\n\n    def compute_class_weights(self):\n        class_counts = np.unique(self.targets, return_counts=True)[-1]\n        frequencies = class_counts / len(self.targets)\n        weights = np.round(1 / frequencies, 2)\n        return torch.FloatTensor(weights / weights.sum()).to(device)","metadata":{"id":"4abc2004","execution":{"iopub.status.busy":"2024-10-28T21:37:38.589514Z","iopub.execute_input":"2024-10-28T21:37:38.590404Z","iopub.status.idle":"2024-10-28T21:37:38.603830Z","shell.execute_reply.started":"2024-10-28T21:37:38.590355Z","shell.execute_reply":"2024-10-28T21:37:38.602899Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Create Data Loaders for MolHIV","metadata":{"id":"8f15487e-5c77-44ad-83f9-aa9df8da20b4"}},{"cell_type":"code","source":"batch_size = 32\n\nmolHIV = PygGraphPropPredDataset(name=\"ogbg-molhiv\", root=\"dataset/\")\nsplit_idx = molHIV.get_idx_split()\n\n# vectorized dataset attempt\ngraph_dataset = GraphDataSetVectorized(molHIV)\natoms_to_index = graph_dataset.atoms_to_index\n\ntrain_dataset = torch.utils.data.Subset(graph_dataset, split_idx[\"train\"])\nval_dataset = torch.utils.data.Subset(graph_dataset, split_idx[\"valid\"])\ntest_dataset = torch.utils.data.Subset(graph_dataset, split_idx[\"test\"])\n\n# Create DataLoaders\ntrain_loader = torch.utils.data.DataLoader(\n    train_dataset, batch_size=batch_size, shuffle=True, num_workers=4\n)\nval_loader = torch.utils.data.DataLoader(\n    val_dataset, batch_size=batch_size, shuffle=False, num_workers=4\n)\ntest_loader = torch.utils.data.DataLoader(\n    test_dataset, batch_size=batch_size, shuffle=False, num_workers=4\n)","metadata":{"id":"a1fd2530-1131-4041-882e-4e73970a6f0b","outputId":"d70019ae-0bbc-4b6c-dcb0-078ce281b68e","execution":{"iopub.status.busy":"2024-10-28T21:37:40.388396Z","iopub.execute_input":"2024-10-28T21:37:40.389143Z","iopub.status.idle":"2024-10-28T21:37:48.021779Z","shell.execute_reply.started":"2024-10-28T21:37:40.389103Z","shell.execute_reply":"2024-10-28T21:37:48.020761Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Model and Training for MolHIV\n\nThe evaluation of MolHIV (and all other datasets from ogb) should happen through an Evaluator. You can also try playing around with learning rate schedulers.","metadata":{"id":"db11527f"}},{"cell_type":"code","source":"evaluator = Evaluator(name='ogbg-molhiv')\n\ndef evaluate(model, loader):\n    model.eval()\n\n    y_true = list()\n    y_pred = list()\n\n    for adjacencies, features, targets in loader:\n        adjacencies, features = adjacencies.to(device), features.to(device)\n\n        with torch.no_grad():\n            pred = model(features, adjacencies)\n        y_pred.append(pred.argmax(dim=-1, keepdims=True))\n        y_true.append(targets)\n\n    y_true = torch.cat(y_true, dim=0).detach().cpu()\n    y_pred = torch.cat(y_pred, dim=0).detach().cpu()\n\n    input_dict = {\"y_true\": y_true.reshape(-1, 1), \"y_pred\": y_pred.reshape(-1,1)}\n\n    return evaluator.eval(input_dict)['rocauc']","metadata":{"id":"52980ac9","execution":{"iopub.status.busy":"2024-10-28T21:37:51.453602Z","iopub.execute_input":"2024-10-28T21:37:51.454427Z","iopub.status.idle":"2024-10-28T21:37:51.465819Z","shell.execute_reply.started":"2024-10-28T21:37:51.454384Z","shell.execute_reply":"2024-10-28T21:37:51.464984Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class EarlyStopping:\n    def __init__(self, patience=20, min_delta=0):\n        \"\"\"\n        Early stopping to prevent overfitting\n\n        Args:\n            patience: How many epochs to wait before stopping after last improvement\n            min_delta: Minimum change to qualify as an improvement\n        \"\"\"\n        self.patience = patience\n        self.min_delta = min_delta\n        self.counter = 0\n        self.best_val = None\n        self.should_stop = False\n\n    def __call__(self, val_score):\n        if self.best_val is None:\n            self.best_val = val_score\n        elif val_score <= self.best_val + self.min_delta:\n            self.counter += 1\n            if self.counter >= self.patience:\n                self.should_stop = True\n        else:\n            self.best_val = val_score\n            self.counter = 0","metadata":{"execution":{"iopub.status.busy":"2024-10-28T21:37:54.545089Z","iopub.execute_input":"2024-10-28T21:37:54.545887Z","iopub.status.idle":"2024-10-28T21:37:54.552705Z","shell.execute_reply.started":"2024-10-28T21:37:54.545842Z","shell.execute_reply":"2024-10-28T21:37:54.551722Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# training with add early stopping\ndef train_model(model, train_loader, val_loader, device, lr=1e-3, weight_decay=1e-4, num_epochs=50, patience=20):\n    # optimizer and rate scheduler\n    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n        optimizer, mode=\"max\", factor=0.5, patience=10, verbose=True\n    )\n\n    # class weights and loss function\n    class_weights = train_loader.dataset.dataset.compute_class_weights()\n    loss_fn = torch.nn.CrossEntropyLoss(weight=class_weights)\n\n    early_stopping = EarlyStopping(patience=patience)\n    best_val_roc = 0\n    best_model = None\n\n    train_losses = []\n    train_aucs = []\n    val_aucs = []\n    \n    for epoch in range(num_epochs):\n        model.train()\n        train_loss = 0\n\n        for adjacencies, features, targets in train_loader:\n            adjacencies, features, targets = (\n                adjacencies.to(device),\n                features.to(device),\n                targets.to(device),\n            )\n            # Forward pass\n            optimizer.zero_grad()\n            logits = model(features, adjacencies)\n            loss = loss_fn(logits, targets.squeeze())\n            # Backward pass\n            loss.backward()\n            optimizer.step()\n\n            train_loss += loss.item()\n        \n        train_loss = train_loss / len(train_loader)\n        train_losses.append(train_loss)\n\n        # Evaluate on train and validation sets\n        train_roc = evaluate(model, train_loader)\n        val_roc = evaluate(model, val_loader)\n\n        train_aucs.append(train_roc)\n        val_aucs.append(val_roc)\n\n        # Print metrics\n        print(f'Epoch {epoch+1:03d} | Train Loss: {train_loss:.4f} | Train ROC: {train_roc:.4f} | Val ROC: {val_roc:.4f}')\n\n        # Learning rate scheduling\n        scheduler.step(val_roc)\n\n        # Save best model and check early stopping\n        if val_roc > best_val_roc:\n            best_val_roc = val_roc\n            best_model = model.state_dict()\n\n        early_stopping(val_roc)\n        if early_stopping.should_stop:\n            print(f'Early stopping triggered at epoch {epoch+1}')\n            break\n\n    return best_model, train_losses, train_aucs, val_aucs","metadata":{"execution":{"iopub.status.busy":"2024-10-28T21:40:45.794519Z","iopub.execute_input":"2024-10-28T21:40:45.794924Z","iopub.status.idle":"2024-10-28T21:40:45.806914Z","shell.execute_reply.started":"2024-10-28T21:40:45.794884Z","shell.execute_reply":"2024-10-28T21:40:45.806056Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = GraphGCN(\n    num_layers=2,\n    in_features=graph_dataset.num_features(),\n    hidden_features=64,\n    out_features=molHIV.num_classes,\n    pooling=MeanPooling(dim=1),\n    activation=torch.nn.functional.relu,\n    skip_connection=True,\n).to(device)\n\nbest_model, train_losses, train_aucs, val_aucs = train_model(model, train_loader, val_loader, device, num_epochs=50, patience=10)","metadata":{"execution":{"iopub.status.busy":"2024-10-28T21:58:09.373053Z","iopub.execute_input":"2024-10-28T21:58:09.374116Z","iopub.status.idle":"2024-10-28T22:12:05.276213Z","shell.execute_reply.started":"2024-10-28T21:58:09.374058Z","shell.execute_reply":"2024-10-28T22:12:05.274973Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot loss and ROC-AUC\nplt.figure(figsize=(12, 4))\nplt.subplot(1, 2, 1)\nplt.plot(train_losses, label='Train Loss')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.legend()\n\nplt.subplot(1, 2, 2)\nplt.plot(train_aucs, label='Train ROC-AUC')\nplt.plot(val_aucs, label='Val ROC-AUC')\nplt.xlabel('Epoch')\nplt.ylabel('ROC-AUC')\nplt.legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-10-28T22:12:19.150970Z","iopub.execute_input":"2024-10-28T22:12:19.151841Z","iopub.status.idle":"2024-10-28T22:12:19.570926Z","shell.execute_reply.started":"2024-10-28T22:12:19.151792Z","shell.execute_reply":"2024-10-28T22:12:19.569963Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Evaluate on test set\ntest_rocauc = evaluate(model, test_loader)\nprint(f\"Test ROC-AUC: {test_rocauc:.4f}\")","metadata":{"id":"7dc0a868","outputId":"f5dac34b-b86c-4656-cfe5-755ae0d2638e","execution":{"iopub.status.busy":"2024-10-28T22:12:21.601111Z","iopub.execute_input":"2024-10-28T22:12:21.601506Z","iopub.status.idle":"2024-10-28T22:12:25.218801Z","shell.execute_reply.started":"2024-10-28T22:12:21.601466Z","shell.execute_reply":"2024-10-28T22:12:25.217409Z"},"trusted":true},"execution_count":null,"outputs":[]}]}